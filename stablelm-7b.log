2024-04-29:11:36:05,615 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:36:09,936 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:36:09,938 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:36:09,938 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False}
2024-04-29:11:36:10,031 INFO     [huggingface.py:164] Using device 'cuda'
auto auto {'load_in_8bit': False, 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/utils/import_utils.py", line 1510, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/nlyaly/miniconda3/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 45, in <module>
    from flash_attn import flash_attn_func, flash_attn_varlen_func
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py", line 10, in <module>
    import flash_attn_2_cuda as flash_attn_cuda
ImportError: /home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/flash_attn_2_cuda.cpython-39-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 562, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 383, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 734, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 748, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 692, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/utils/import_utils.py", line 1500, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/utils/import_utils.py", line 1512, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt_neox.modeling_gpt_neox because of the following error (look up to see its traceback):
/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/flash_attn_2_cuda.cpython-39-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi
2024-04-29:11:36:59,416 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:37:03,483 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:37:03,484 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:37:03,484 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False}
2024-04-29:11:37:03,578 INFO     [huggingface.py:164] Using device 'cuda'
auto auto {'load_in_8bit': False, 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.13s/it]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 
2024-04-29:11:37:38,621 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:37:42,704 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:37:42,705 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:37:42,705 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False}
2024-04-29:11:37:42,789 INFO     [huggingface.py:164] Using device 'cuda'
auto auto {'load_in_8bit': False, 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.12s/it]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3531, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3958, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 812, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 
2024-04-29:11:38:27,482 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:38:31,543 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:38:31,544 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:38:31,544 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'torch_dtype': 'torch.bfloat16'}
2024-04-29:11:38:31,657 INFO     [huggingface.py:164] Using device 'cuda'
auto auto {'load_in_8bit': False, 'torch_dtype': 'torch.bfloat16', 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
TypeError: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'torch_dtype'
2024-04-29:11:42:10,893 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:42:15,039 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:42:15,040 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:42:15,040 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'device': 'auto'}
2024-04-29:11:42:15,125 INFO     [huggingface.py:172] Device not specified
2024-04-29:11:42:15,125 INFO     [huggingface.py:173] Cuda Available? True
auto auto {'load_in_8bit': False, 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.12s/it]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3531, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3958, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 812, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 
2024-04-29:11:43:04,499 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:43:08,574 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:43:08,575 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:43:08,575 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'limit': 2}
2024-04-29:11:43:08,690 INFO     [huggingface.py:164] Using device 'cuda'
auto auto {'load_in_8bit': False, 'limit': 2, 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3404, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
TypeError: __init__() got an unexpected keyword argument 'limit'
2024-04-29:11:44:12,489 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:44:16,667 WARNING  [__main__.py:266]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-04-29:11:44:16,668 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:44:16,669 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:44:16,669 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False}
2024-04-29:11:44:16,771 INFO     [huggingface.py:164] Using device 'cuda'
auto auto {'load_in_8bit': False, 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.11s/it]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3531, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3958, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 812, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 
2024-04-29:11:59:08,244 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:59:12,433 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:59:12,434 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:59:12,434 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True}
2024-04-29:11:59:12,529 INFO     [huggingface.py:164] Using device 'cuda'
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
auto auto {'load_in_8bit': True, 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.67s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.81s/it]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3531, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3958, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 814, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/quantizers/quantizer_bnb_8bit.py", line 205, in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/nn/modules.py", line 606, in to
    return self.cuda(device)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/nn/modules.py", line 579, in cuda
    CB, CBt, SCB, SCBt, coo_tensorB = bnb.functional.double_quant(B)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/functional.py", line 2522, in double_quant
    out_col = torch.zeros(A.shape, device=device, dtype=torch.int8)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 
2024-04-29:11:59:32,296 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:11:59:36,364 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:11:59:36,364 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:11:59:36,364 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True}
2024-04-29:11:59:36,449 INFO     [huggingface.py:164] Using device 'cuda'
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
auto auto {'load_in_8bit': True, 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
2024-04-29:11:59:42,791 WARNING  [big_modeling.py:450] You shouldn't move a model that is dispatched using accelerate hooks.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-29:11:59:43,037 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:11:59:43,037 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:11:59:43,037 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:11:59:43,038 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:11:59:43,038 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-04-29:11:59:43,038 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:11:59:46,351 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:11:59:46,437 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 859.29it/s]
2024-04-29:11:59:46,512 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|▏         | 1/62 [00:00<00:36,  1.69it/s]  2%|▏         | 1/62 [00:01<01:25,  1.40s/it]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 251, in simple_evaluate
    results = evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 390, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 899, in loglikelihood_rolling
    string_nll = self._loglikelihood_tokens(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 1092, in _loglikelihood_tokens
    self._model_call(batched_inps, **call_kwargs), dim=-1
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 807, in _model_call
    return self.model(inps).logits
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 1036, in forward
    outputs = self.gpt_neox(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 927, in forward
    outputs = layer(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 692, in forward
    attention_layer_outputs = self.attention(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 213, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 270, in _attn
    attn_scores = torch.baddbmm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 
2024-04-29:12:00:41,422 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:00:45,494 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:12:00:45,495 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:12:00:45,495 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True, 'attn_implementation': 'flash_attention_2'}
2024-04-29:12:00:45,583 INFO     [huggingface.py:164] Using device 'cuda'
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
auto auto {'load_in_8bit': True, 'attn_implementation': 'flash_attention_2', 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3398, in from_pretrained
    config = cls._autoset_attn_implementation(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 1377, in _autoset_attn_implementation
    cls._check_and_enable_flash_attn_2(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/modeling_utils.py", line 1469, in _check_and_enable_flash_attn_2
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.
2024-04-29:12:02:39,879 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:02:43,957 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:12:02:43,958 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:12:02:43,958 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True, 'attn_implementation': 'flash_attention_2'}
2024-04-29:12:02:44,041 INFO     [huggingface.py:164] Using device 'cuda'
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in GPTNeoXForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in GPTNeoXModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
auto auto {'load_in_8bit': True, 'attn_implementation': 'flash_attention_2', 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
2024-04-29:12:02:50,013 WARNING  [big_modeling.py:450] You shouldn't move a model that is dispatched using accelerate hooks.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-29:12:02:50,248 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:12:02:50,248 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:12:02:50,248 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:12:02:50,248 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:12:02:50,248 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-04-29:12:02:50,248 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:12:02:53,704 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:12:02:53,750 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 864.34it/s]
2024-04-29:12:02:53,825 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float32.
  0%|          | 0/62 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 251, in simple_evaluate
    results = evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 390, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 899, in loglikelihood_rolling
    string_nll = self._loglikelihood_tokens(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 1092, in _loglikelihood_tokens
    self._model_call(batched_inps, **call_kwargs), dim=-1
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 807, in _model_call
    return self.model(inps).logits
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 1036, in forward
    outputs = self.gpt_neox(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 927, in forward
    outputs = layer(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 692, in forward
    attention_layer_outputs = self.attention(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 408, in forward
    attn_weights = self._flash_attention_forward(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 478, in _flash_attention_forward
    attn_output = flash_attn_func(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py", line 831, in flash_attn_func
    return FlashAttnFunc.apply(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py", line 511, in forward
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py", line 51, in _flash_attn_forward
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(
RuntimeError: FlashAttention only support fp16 and bf16 data type
2024-04-29:12:04:44,828 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:05:10,262 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:05:14,417 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:12:05:14,418 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:12:05:14,418 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True, 'torch_dtype': 'torch.float16', 'attn_implementation': 'flash_attention_2'}
2024-04-29:12:05:14,500 INFO     [huggingface.py:164] Using device 'cuda'
auto auto {'load_in_8bit': True, 'torch_dtype': 'torch.float16', 'attn_implementation': 'flash_attention_2', 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
TypeError: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'torch_dtype'
2024-04-29:12:05:57,532 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:06:01,624 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:12:06:01,625 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:12:06:01,625 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True, 'torch_dtype': 'torch.float16', 'attn_implementation': 'flash_attention_2'}
2024-04-29:12:06:01,710 INFO     [huggingface.py:164] Using device 'cuda'
auto auto {'load_in_8bit': True, 'torch_dtype': 'torch.float16', 'attn_implementation': 'flash_attention_2', 'device_map': {'': 'cuda'}} cuda
default args: 
 {}
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
TypeError: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'torch_dtype'
2024-04-29:12:07:44,010 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:07:48,081 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:12:07:48,082 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:12:07:48,082 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True, 'torch_dtype': 'torch.float16', 'attn_implementation': 'flash_attention_2'}
2024-04-29:12:07:48,169 INFO     [huggingface.py:164] Using device 'cuda'
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 554, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
TypeError: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'torch_dtype'
2024-04-29:12:08:38,090 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:08:42,218 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:12:08:42,219 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:12:08:42,219 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True, 'dtype': 'torch.float16', 'attn_implementation': 'flash_attention_2'}
2024-04-29:12:08:42,324 INFO     [huggingface.py:164] Using device 'cuda'
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 203, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 557, in _create_model
    torch_dtype=get_dtype(dtype),
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/utils.py", line 202, in get_dtype
    _torch_dtype = getattr(torch, dtype)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/__init__.py", line 2003, in __getattr__
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
AttributeError: module 'torch' has no attribute 'torch.float16'
2024-04-29:12:09:48,320 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:09:52,411 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:12:09:52,412 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:12:09:52,412 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': True, 'dtype': 'float16', 'attn_implementation': 'flash_attention_2'}
2024-04-29:12:09:52,506 INFO     [huggingface.py:164] Using device 'cuda'
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
2024-04-29:12:09:58,358 WARNING  [big_modeling.py:450] You shouldn't move a model that is dispatched using accelerate hooks.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-29:12:09:58,621 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:12:09:58,621 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:12:09:58,621 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:12:09:58,621 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:12:09:58,621 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-04-29:12:09:58,621 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:12:10:01,591 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:12:10:01,636 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 856.45it/s]
2024-04-29:12:10:01,711 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  2%|▏         | 1/62 [00:00<00:24,  2.45it/s]  3%|▎         | 2/62 [00:01<00:55,  1.09it/s]  5%|▍         | 3/62 [00:02<00:40,  1.46it/s]  6%|▋         | 4/62 [00:03<00:53,  1.09it/s]  8%|▊         | 5/62 [00:03<00:40,  1.41it/s] 10%|▉         | 6/62 [00:04<00:33,  1.66it/s] 11%|█▏        | 7/62 [00:06<00:56,  1.03s/it] 13%|█▎        | 8/62 [00:06<00:44,  1.22it/s] 15%|█▍        | 9/62 [00:08<01:12,  1.36s/it] 16%|█▌        | 10/62 [00:09<00:53,  1.02s/it] 18%|█▊        | 11/62 [00:09<00:44,  1.15it/s] 19%|█▉        | 12/62 [00:11<00:59,  1.18s/it] 21%|██        | 13/62 [00:11<00:42,  1.14it/s] 23%|██▎       | 14/62 [00:12<00:34,  1.38it/s] 24%|██▍       | 15/62 [00:12<00:25,  1.86it/s] 26%|██▌       | 16/62 [00:12<00:21,  2.19it/s] 27%|██▋       | 17/62 [00:13<00:31,  1.42it/s] 29%|██▉       | 18/62 [00:13<00:23,  1.83it/s] 31%|███       | 19/62 [00:15<00:32,  1.31it/s] 32%|███▏      | 20/62 [00:16<00:38,  1.09it/s] 34%|███▍      | 21/62 [00:17<00:41,  1.02s/it] 35%|███▌      | 22/62 [00:18<00:31,  1.28it/s] 37%|███▋      | 23/62 [00:19<00:36,  1.08it/s] 39%|███▊      | 24/62 [00:21<00:53,  1.42s/it] 40%|████      | 25/62 [00:22<00:41,  1.11s/it] 42%|████▏     | 26/62 [00:23<00:41,  1.16s/it] 44%|████▎     | 27/62 [00:25<00:48,  1.39s/it] 45%|████▌     | 28/62 [00:25<00:35,  1.04s/it] 48%|████▊     | 30/62 [00:25<00:18,  1.69it/s] 50%|█████     | 31/62 [00:27<00:23,  1.31it/s] 52%|█████▏    | 32/62 [00:27<00:18,  1.59it/s] 53%|█████▎    | 33/62 [00:28<00:23,  1.24it/s] 55%|█████▍    | 34/62 [00:28<00:18,  1.51it/s] 56%|█████▋    | 35/62 [00:30<00:27,  1.02s/it] 58%|█████▊    | 36/62 [00:32<00:28,  1.10s/it] 60%|█████▉    | 37/62 [00:32<00:21,  1.18it/s] 61%|██████▏   | 38/62 [00:34<00:32,  1.36s/it] 63%|██████▎   | 39/62 [00:36<00:30,  1.33s/it] 65%|██████▍   | 40/62 [00:38<00:33,  1.51s/it] 66%|██████▌   | 41/62 [00:40<00:34,  1.64s/it] 68%|██████▊   | 42/62 [00:40<00:25,  1.28s/it] 69%|██████▉   | 43/62 [00:40<00:18,  1.05it/s] 71%|███████   | 44/62 [00:41<00:18,  1.05s/it] 73%|███████▎  | 45/62 [00:42<00:14,  1.18it/s] 74%|███████▍  | 46/62 [00:42<00:10,  1.54it/s] 76%|███████▌  | 47/62 [00:42<00:07,  1.93it/s] 77%|███████▋  | 48/62 [00:44<00:10,  1.34it/s] 79%|███████▉  | 49/62 [00:44<00:07,  1.69it/s] 81%|████████  | 50/62 [00:44<00:05,  2.03it/s] 82%|████████▏ | 51/62 [00:44<00:05,  2.06it/s] 84%|████████▍ | 52/62 [00:45<00:04,  2.09it/s] 85%|████████▌ | 53/62 [00:45<00:03,  2.59it/s] 87%|████████▋ | 54/62 [00:45<00:02,  3.09it/s] 89%|████████▊ | 55/62 [00:45<00:01,  3.62it/s] 90%|█████████ | 56/62 [00:46<00:01,  4.13it/s] 92%|█████████▏| 57/62 [00:47<00:02,  1.80it/s] 94%|█████████▎| 58/62 [00:47<00:01,  2.35it/s] 95%|█████████▌| 59/62 [00:48<00:01,  2.13it/s] 97%|█████████▋| 60/62 [00:49<00:01,  1.40it/s] 98%|█████████▊| 61/62 [00:50<00:00,  1.13it/s]100%|██████████| 62/62 [00:51<00:00,  1.24it/s]100%|██████████| 62/62 [00:51<00:00,  1.21it/s]
hf (pretrained=stabilityai/stablelm-tuned-alpha-7b,trust_remote_code=True,load_in_8bit=True,dtype=float16,attn_implementation=flash_attention_2), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
| Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
|--------|------:|------|-----:|---------------|------:|---|------|
|wikitext|      2|none  |     0|word_perplexity|22.4435|±  |N/A   |
|        |       |none  |     0|byte_perplexity| 1.7892|±  |N/A   |
|        |       |none  |     0|bits_per_byte  | 0.8393|±  |N/A   |

2024-04-29:12:12:19,805 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:12:12:23,994 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:12:12:23,995 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:12:12:23,995 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'dtype': 'float16', 'attn_implementation': 'flash_attention_2'}
2024-04-29:12:12:24,082 INFO     [huggingface.py:164] Using device 'cuda'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-29:12:12:30,109 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:12:12:30,109 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:12:12:30,109 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:12:12:30,109 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:12:12:30,109 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-04-29:12:12:30,109 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:12:12:33,212 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:12:12:33,258 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 845.28it/s]
2024-04-29:12:12:33,334 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  2%|▏         | 1/62 [00:00<00:26,  2.29it/s]  3%|▎         | 2/62 [00:02<01:20,  1.35s/it]  5%|▍         | 3/62 [00:03<01:00,  1.02s/it]  6%|▋         | 4/62 [00:05<01:21,  1.40s/it]  8%|▊         | 5/62 [00:05<01:02,  1.09s/it] 10%|▉         | 6/62 [00:06<00:52,  1.06it/s] 11%|█▏        | 7/62 [00:09<01:28,  1.61s/it] 13%|█▎        | 8/62 [00:09<01:09,  1.29s/it] 15%|█▍        | 9/62 [00:13<01:52,  2.13s/it] 16%|█▌        | 10/62 [00:14<01:23,  1.61s/it] 18%|█▊        | 11/62 [00:15<01:10,  1.37s/it] 19%|█▉        | 12/62 [00:18<01:33,  1.87s/it] 21%|██        | 13/62 [00:18<01:07,  1.38s/it] 23%|██▎       | 14/62 [00:18<00:55,  1.15s/it] 24%|██▍       | 15/62 [00:19<00:39,  1.18it/s] 26%|██▌       | 16/62 [00:19<00:33,  1.37it/s] 27%|██▋       | 17/62 [00:21<00:50,  1.11s/it] 29%|██▉       | 18/62 [00:21<00:38,  1.15it/s] 31%|███       | 19/62 [00:23<00:52,  1.21s/it] 32%|███▏      | 20/62 [00:25<01:00,  1.45s/it] 34%|███▍      | 21/62 [00:27<01:06,  1.62s/it] 35%|███▌      | 22/62 [00:28<00:49,  1.24s/it] 37%|███▋      | 23/62 [00:30<00:57,  1.47s/it] 39%|███▊      | 24/62 [00:34<01:25,  2.24s/it] 40%|████      | 25/62 [00:34<01:05,  1.77s/it] 42%|████▏     | 26/62 [00:36<01:06,  1.84s/it] 44%|████▎     | 27/62 [00:40<01:17,  2.20s/it] 45%|████▌     | 28/62 [00:40<00:55,  1.65s/it] 48%|████▊     | 30/62 [00:40<00:29,  1.09it/s] 50%|█████     | 31/62 [00:42<00:36,  1.19s/it] 52%|█████▏    | 32/62 [00:42<00:29,  1.01it/s] 53%|█████▎    | 33/62 [00:45<00:37,  1.28s/it] 55%|█████▍    | 34/62 [00:45<00:29,  1.06s/it] 56%|█████▋    | 35/62 [00:48<00:43,  1.63s/it] 58%|█████▊    | 36/62 [00:50<00:45,  1.75s/it] 60%|█████▉    | 37/62 [00:51<00:34,  1.36s/it] 61%|██████▏   | 38/62 [00:55<00:52,  2.17s/it] 63%|██████▎   | 39/62 [00:57<00:49,  2.13s/it] 65%|██████▍   | 40/62 [01:00<00:53,  2.41s/it] 66%|██████▌   | 41/62 [01:03<00:54,  2.61s/it] 68%|██████▊   | 42/62 [01:04<00:40,  2.04s/it] 69%|██████▉   | 43/62 [01:04<00:29,  1.53s/it] 71%|███████   | 44/62 [01:06<00:30,  1.69s/it] 73%|███████▎  | 45/62 [01:07<00:23,  1.37s/it] 74%|███████▍  | 46/62 [01:07<00:16,  1.05s/it] 76%|███████▌  | 47/62 [01:07<00:12,  1.18it/s] 77%|███████▋  | 48/62 [01:09<00:16,  1.21s/it] 79%|███████▉  | 49/62 [01:10<00:12,  1.04it/s] 81%|████████  | 50/62 [01:10<00:09,  1.24it/s] 82%|████████▏ | 51/62 [01:11<00:08,  1.27it/s] 84%|████████▍ | 52/62 [01:12<00:07,  1.28it/s] 85%|████████▌ | 53/62 [01:12<00:05,  1.59it/s] 87%|████████▋ | 54/62 [01:12<00:04,  1.88it/s] 89%|████████▊ | 55/62 [01:12<00:03,  2.21it/s] 90%|█████████ | 56/62 [01:13<00:02,  2.51it/s] 92%|█████████▏| 57/62 [01:15<00:04,  1.12it/s] 94%|█████████▎| 58/62 [01:15<00:02,  1.47it/s] 95%|█████████▌| 59/62 [01:16<00:02,  1.34it/s] 97%|█████████▋| 60/62 [01:18<00:02,  1.14s/it] 98%|█████████▊| 61/62 [01:20<00:01,  1.41s/it]100%|██████████| 62/62 [01:21<00:00,  1.28s/it]100%|██████████| 62/62 [01:21<00:00,  1.31s/it]
hf (pretrained=stabilityai/stablelm-tuned-alpha-7b,trust_remote_code=True,load_in_8bit=False,dtype=float16,attn_implementation=flash_attention_2), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
| Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
|--------|------:|------|-----:|---------------|------:|---|------|
|wikitext|      2|none  |     0|word_perplexity|22.3175|±  |N/A   |
|        |       |none  |     0|byte_perplexity| 1.7873|±  |N/A   |
|        |       |none  |     0|bits_per_byte  | 0.8378|±  |N/A   |

2024-04-29:14:20:58,187 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:14:21:02,268 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:14:21:02,269 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:14:21:02,269 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'dtype': 'float16'}
2024-04-29:14:21:02,353 INFO     [huggingface.py:164] Using device 'cuda'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-29:14:21:08,273 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:14:21:08,273 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:14:21:08,273 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:14:21:08,273 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:14:21:08,273 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-04-29:14:21:08,273 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:14:21:11,253 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:14:21:11,322 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 847.23it/s]
2024-04-29:14:21:11,398 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  2%|▏         | 1/62 [00:00<00:30,  2.01it/s]  2%|▏         | 1/62 [00:00<00:43,  1.42it/s]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 251, in simple_evaluate
    results = evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 390, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 899, in loglikelihood_rolling
    string_nll = self._loglikelihood_tokens(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 1092, in _loglikelihood_tokens
    self._model_call(batched_inps, **call_kwargs), dim=-1
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 807, in _model_call
    return self.model(inps).logits
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 1036, in forward
    outputs = self.gpt_neox(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 927, in forward
    outputs = layer(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 692, in forward
    attention_layer_outputs = self.attention(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 213, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 290, in _attn
    attn_weights = attn_weights.to(value.dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 
2024-04-29:14:21:46,411 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-29:14:21:50,542 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-29:14:21:50,543 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-29:14:21:50,543 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'dtype': 'float16', 'device_map': 'auto'}
2024-04-29:14:21:50,634 INFO     [huggingface.py:164] Using device 'cuda'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
2024-04-29:14:21:56,940 WARNING  [big_modeling.py:450] You shouldn't move a model that is dispatched using accelerate hooks.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-29:14:21:59,229 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:14:21:59,229 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:14:21:59,229 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-29:14:21:59,229 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-29:14:21:59,229 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-04-29:14:21:59,229 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:14:22:02,485 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-04-29:14:22:02,532 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 662.02it/s]
2024-04-29:14:22:02,628 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  0%|          | 0/62 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 251, in simple_evaluate
    results = evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 390, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 899, in loglikelihood_rolling
    string_nll = self._loglikelihood_tokens(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 1092, in _loglikelihood_tokens
    self._model_call(batched_inps, **call_kwargs), dim=-1
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 807, in _model_call
    return self.model(inps).logits
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 1036, in forward
    outputs = self.gpt_neox(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 927, in forward
    outputs = layer(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 693, in forward
    self.input_layernorm(hidden_states),
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
    return F.layer_norm(
  File "/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)
2024-04-30:15:26:07,462 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-30:15:26:11,497 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-30:15:26:11,499 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-30:15:26:11,499 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'dtype': 'float16', 'peft_dir': '/home/nlyaly/projects/lm-eval-2/cache/fp32/opt_search_wikitext2_loftq_init_all_R8_Lddq/15'}
2024-04-30:15:26:11,581 INFO     [huggingface.py:168] Using device 'cuda'
/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:122: UserWarning: 

================================================================================
WARNING: Manual override via BNB_CUDA_VERSION env variable detected!
BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64
Loading: libbitsandbytes_cuda124.so
================================================================================


  warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.22s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-30:15:26:30,313 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-30:15:26:30,313 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-30:15:26:30,313 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-30:15:26:30,313 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-30:15:26:30,313 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-04-30:15:26:30,313 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-04-30:15:26:33,119 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-04-30:15:26:33,165 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 860.97it/s]
2024-04-30:15:26:33,240 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  2%|▏         | 1/62 [00:00<00:34,  1.78it/s]  3%|▎         | 2/62 [00:05<02:56,  2.93s/it]  5%|▍         | 3/62 [00:06<02:09,  2.20s/it]  6%|▋         | 4/62 [00:11<03:02,  3.15s/it]  8%|▊         | 5/62 [00:12<02:16,  2.39s/it] 10%|▉         | 6/62 [00:13<01:53,  2.02s/it] 11%|█▏        | 7/62 [00:20<03:19,  3.63s/it] 13%|█▎        | 8/62 [00:21<02:34,  2.86s/it] 15%|█▍        | 9/62 [00:30<04:18,  4.87s/it] 16%|█▌        | 10/62 [00:31<03:08,  3.63s/it] 18%|█▊        | 11/62 [00:33<02:36,  3.08s/it] 19%|█▉        | 12/62 [00:40<03:34,  4.29s/it] 21%|██        | 13/62 [00:41<02:33,  3.13s/it] 23%|██▎       | 14/62 [00:42<02:02,  2.56s/it] 24%|██▍       | 15/62 [00:42<01:27,  1.86s/it] 26%|██▌       | 16/62 [00:43<01:11,  1.56s/it] 27%|██▋       | 17/62 [00:48<01:53,  2.51s/it] 29%|██▉       | 18/62 [00:48<01:23,  1.91s/it] 31%|███       | 19/62 [00:53<01:58,  2.76s/it] 32%|███▏      | 20/62 [00:58<02:20,  3.35s/it] 34%|███▍      | 21/62 [01:02<02:34,  3.77s/it] 35%|███▌      | 22/62 [01:03<01:53,  2.83s/it] 37%|███▋      | 23/62 [01:08<02:13,  3.41s/it] 39%|███▊      | 24/62 [01:17<03:19,  5.25s/it] 40%|████      | 25/62 [01:19<02:30,  4.07s/it] 42%|████▏     | 26/62 [01:23<02:34,  4.29s/it] 44%|████▎     | 27/62 [01:31<03:00,  5.16s/it] 45%|████▌     | 28/62 [01:31<02:08,  3.79s/it] 48%|████▊     | 30/62 [01:31<01:07,  2.10s/it] 50%|█████     | 31/62 [01:36<01:25,  2.77s/it] 52%|█████▏    | 32/62 [01:37<01:07,  2.24s/it] 53%|█████▎    | 33/62 [01:42<01:25,  2.94s/it] 55%|█████▍    | 34/62 [01:43<01:06,  2.37s/it] 56%|█████▋    | 35/62 [01:50<01:41,  3.75s/it] 58%|█████▊    | 36/62 [01:55<01:45,  4.06s/it] 60%|█████▉    | 37/62 [01:56<01:17,  3.10s/it] 61%|██████▏   | 38/62 [02:05<02:00,  5.03s/it] 63%|██████▎   | 39/62 [02:10<01:54,  4.97s/it] 65%|██████▍   | 40/62 [02:17<02:04,  5.65s/it] 66%|██████▌   | 41/62 [02:25<02:08,  6.13s/it] 68%|██████▊   | 42/62 [02:26<01:34,  4.75s/it] 69%|██████▉   | 43/62 [02:27<01:06,  3.50s/it] 71%|███████   | 44/62 [02:31<01:10,  3.90s/it] 73%|███████▎  | 45/62 [02:33<00:52,  3.11s/it] 74%|███████▍  | 46/62 [02:33<00:37,  2.33s/it] 76%|███████▌  | 47/62 [02:34<00:27,  1.82s/it] 77%|███████▋  | 48/62 [02:39<00:38,  2.73s/it] 79%|███████▉  | 49/62 [02:39<00:27,  2.12s/it] 81%|████████  | 50/62 [02:40<00:20,  1.72s/it] 82%|████████▏ | 51/62 [02:42<00:18,  1.70s/it] 84%|████████▍ | 52/62 [02:43<00:16,  1.68s/it] 85%|████████▌ | 53/62 [02:44<00:11,  1.32s/it] 87%|████████▋ | 54/62 [02:44<00:08,  1.08s/it] 89%|████████▊ | 55/62 [02:45<00:06,  1.12it/s] 90%|█████████ | 56/62 [02:45<00:04,  1.31it/s] 92%|█████████▏| 57/62 [02:50<00:09,  1.99s/it] 94%|█████████▎| 58/62 [02:51<00:05,  1.48s/it] 95%|█████████▌| 59/62 [02:53<00:04,  1.66s/it] 97%|█████████▋| 60/62 [02:57<00:05,  2.62s/it] 98%|█████████▊| 61/62 [03:02<00:03,  3.28s/it]100%|██████████| 62/62 [03:05<00:00,  2.99s/it]100%|██████████| 62/62 [03:05<00:00,  2.99s/it]
hf (pretrained=stabilityai/stablelm-tuned-alpha-7b,trust_remote_code=True,load_in_8bit=False,dtype=float16,peft_dir=/home/nlyaly/projects/lm-eval-2/cache/fp32/opt_search_wikitext2_loftq_init_all_R8_Lddq/15), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
| Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
|--------|------:|------|-----:|---------------|------:|---|------|
|wikitext|      2|none  |     0|word_perplexity|22.8119|±  |N/A   |
|        |       |none  |     0|byte_perplexity| 1.7947|±  |N/A   |
|        |       |none  |     0|bits_per_byte  | 0.8437|±  |N/A   |

2024-04-30:15:42:53,842 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-30:15:42:57,932 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-30:15:42:57,933 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-30:15:42:57,933 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'dtype': 'float16', 'peft_dir': '/home/nlyaly/projects/lm-eval-2/cache/stablelm-tuned-alpha-7b/opt_search_synthetic_loftq_init_all_R8_Lddq/15'}
2024-04-30:15:42:58,019 INFO     [huggingface.py:168] Using device 'cuda'
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 207, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 549, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
TypeError: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'device_map'
2024-04-30:15:44:36,568 INFO     [__main__.py:251] Verbosity set to INFO
2024-04-30:15:44:40,612 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-04-30:15:44:40,613 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-04-30:15:44:40,613 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'device': 'cuda:2', 'dtype': 'float16', 'peft_dir': '/home/nlyaly/projects/lm-eval-2/cache/stablelm-tuned-alpha-7b/opt_search_synthetic_loftq_init_all_R8_Lddq/15'}
2024-04-30:15:44:40,706 INFO     [huggingface.py:168] Using device 'cuda:2'
/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:122: UserWarning: 

================================================================================
WARNING: Manual override via BNB_CUDA_VERSION env variable detected!
BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64
Loading: libbitsandbytes_cuda124.so
================================================================================


  warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-30:15:44:50,003 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-30:15:44:50,003 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-30:15:44:50,003 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-04-30:15:44:50,003 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-04-30:15:44:50,003 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-04-30:15:44:50,003 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-04-30:15:44:52,920 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-04-30:15:44:52,965 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 863.30it/s]
2024-04-30:15:44:53,040 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  2%|▏         | 1/62 [00:00<00:33,  1.81it/s]  3%|▎         | 2/62 [00:04<02:48,  2.81s/it]  5%|▍         | 3/62 [00:06<02:03,  2.10s/it]  6%|▋         | 4/62 [00:10<02:54,  3.01s/it]  8%|▊         | 5/62 [00:11<02:10,  2.29s/it] 10%|▉         | 6/62 [00:12<01:48,  1.93s/it] 11%|█▏        | 7/62 [00:19<03:10,  3.46s/it] 13%|█▎        | 8/62 [00:20<02:27,  2.73s/it] 15%|█▍        | 9/62 [00:29<04:06,  4.65s/it] 16%|█▌        | 10/62 [00:30<03:00,  3.47s/it] 18%|█▊        | 11/62 [00:32<02:29,  2.93s/it] 19%|█▉        | 12/62 [00:38<03:24,  4.08s/it] 21%|██        | 13/62 [00:39<02:26,  2.98s/it] 23%|██▎       | 14/62 [00:40<01:56,  2.43s/it] 24%|██▍       | 15/62 [00:40<01:23,  1.77s/it] 26%|██▌       | 16/62 [00:41<01:08,  1.48s/it] 27%|██▋       | 17/62 [00:45<01:47,  2.39s/it] 29%|██▉       | 18/62 [00:46<01:19,  1.82s/it] 31%|███       | 19/62 [00:50<01:52,  2.62s/it] 32%|███▏      | 20/62 [00:55<02:14,  3.19s/it] 34%|███▍      | 21/62 [00:59<02:27,  3.59s/it] 35%|███▌      | 22/62 [01:00<01:47,  2.70s/it] 37%|███▋      | 23/62 [01:05<02:06,  3.25s/it] 39%|███▊      | 24/62 [01:14<03:09,  4.99s/it] 40%|████      | 25/62 [01:15<02:23,  3.88s/it] 42%|████▏     | 26/62 [01:19<02:27,  4.09s/it] 44%|████▎     | 27/62 [01:26<02:52,  4.92s/it] 45%|████▌     | 28/62 [01:27<02:02,  3.62s/it] 48%|████▊     | 30/62 [01:27<01:04,  2.00s/it] 50%|█████     | 31/62 [01:32<01:21,  2.64s/it] 52%|█████▏    | 32/62 [01:32<01:04,  2.14s/it] 53%|█████▎    | 33/62 [01:37<01:21,  2.80s/it] 55%|█████▍    | 34/62 [01:38<01:03,  2.26s/it] 56%|█████▋    | 35/62 [01:45<01:35,  3.56s/it] 58%|█████▊    | 36/62 [01:49<01:39,  3.84s/it] 60%|█████▉    | 37/62 [01:50<01:13,  2.94s/it] 61%|██████▏   | 38/62 [01:59<01:53,  4.74s/it] 63%|██████▎   | 39/62 [02:04<01:47,  4.68s/it] 65%|██████▍   | 40/62 [02:10<01:57,  5.32s/it] 66%|██████▌   | 41/62 [02:17<02:01,  5.76s/it] 68%|██████▊   | 42/62 [02:19<01:29,  4.45s/it] 69%|██████▉   | 43/62 [02:19<01:02,  3.29s/it] 71%|███████   | 44/62 [02:24<01:06,  3.67s/it] 73%|███████▎  | 45/62 [02:25<00:49,  2.91s/it] 74%|███████▍  | 46/62 [02:25<00:34,  2.19s/it] 76%|███████▌  | 47/62 [02:26<00:25,  1.72s/it] 77%|███████▋  | 48/62 [02:31<00:35,  2.57s/it] 79%|███████▉  | 49/62 [02:31<00:25,  2.00s/it] 81%|████████  | 50/62 [02:32<00:19,  1.62s/it] 82%|████████▏ | 51/62 [02:33<00:17,  1.60s/it] 84%|████████▍ | 52/62 [02:35<00:15,  1.57s/it] 85%|████████▌ | 53/62 [02:35<00:11,  1.24s/it] 87%|████████▋ | 54/62 [02:36<00:08,  1.01s/it] 89%|████████▊ | 55/62 [02:36<00:05,  1.19it/s] 90%|█████████ | 56/62 [02:37<00:04,  1.38it/s] 92%|█████████▏| 57/62 [02:41<00:09,  1.88s/it] 94%|█████████▎| 58/62 [02:42<00:05,  1.40s/it] 95%|█████████▌| 59/62 [02:44<00:04,  1.57s/it] 97%|█████████▋| 60/62 [02:48<00:04,  2.47s/it] 98%|█████████▊| 61/62 [02:53<00:03,  3.10s/it]100%|██████████| 62/62 [02:55<00:00,  2.81s/it]100%|██████████| 62/62 [02:55<00:00,  2.83s/it]
hf (pretrained=stabilityai/stablelm-tuned-alpha-7b,trust_remote_code=True,load_in_8bit=False,device=cuda:2,dtype=float16,peft_dir=/home/nlyaly/projects/lm-eval-2/cache/stablelm-tuned-alpha-7b/opt_search_synthetic_loftq_init_all_R8_Lddq/15), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
| Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
|--------|------:|------|-----:|---------------|------:|---|------|
|wikitext|      2|none  |     0|word_perplexity|22.8475|±  |N/A   |
|        |       |none  |     0|byte_perplexity| 1.7952|±  |N/A   |
|        |       |none  |     0|bits_per_byte  | 0.8441|±  |N/A   |

2024-05-02:20:53:40,258 INFO     [__main__.py:251] Verbosity set to INFO
2024-05-02:20:53:44,444 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-05-02:20:53:44,446 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-05-02:20:53:44,446 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-tuned-alpha-7b', 'trust_remote_code': True, 'load_in_8bit': False, 'device': 'cuda:2', 'dtype': 'float16', 'peft_dir': '/home/nlyaly/projects/lm-evaluation-harness/cache/stablelm-2-zephyr-1_6b/opt_search_q1_wikitext2_loftq_init_R8_Ldugqkvo/23'}
2024-05-02:20:53:44,526 INFO     [huggingface.py:168] Using device 'cuda:2'
/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:122: UserWarning: 

================================================================================
WARNING: Manual override via BNB_CUDA_VERSION env variable detected!
BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64
Loading: libbitsandbytes_cuda124.so
================================================================================


  warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 207, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 562, in _create_model
    self._model = PeftModel.from_pretrained(
  File "/home/nlyaly/projects/peft/src/peft/peft_model.py", line 391, in from_pretrained
    model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config, adapter_name)
  File "/home/nlyaly/projects/peft/src/peft/peft_model.py", line 1269, in __init__
    super().__init__(model, peft_config, adapter_name)
  File "/home/nlyaly/projects/peft/src/peft/peft_model.py", line 135, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/home/nlyaly/projects/peft/src/peft/tuners/lora/model.py", line 137, in __init__
    super().__init__(model, config, adapter_name)
  File "/home/nlyaly/projects/peft/src/peft/tuners/tuners_utils.py", line 166, in __init__
    self.inject_adapter(self.model, adapter_name)
  File "/home/nlyaly/projects/peft/src/peft/tuners/tuners_utils.py", line 361, in inject_adapter
    raise ValueError(
ValueError: Target modules {'layers.0.mlp.down_proj', 'layers.20.mlp.down_proj', 'layers.17.mlp.gate_proj', 'layers.18.mlp.down_proj', 'layers.17.self_attn.q_proj', 'layers.14.self_attn.k_proj', 'layers.19.mlp.gate_proj', 'layers.4.self_attn.v_proj', 'layers.16.self_attn.q_proj', 'layers.7.self_attn.v_proj', 'layers.21.mlp.gate_proj', 'layers.10.self_attn.o_proj', 'layers.8.mlp.gate_proj', 'layers.14.self_attn.o_proj', 'layers.12.mlp.gate_proj', 'layers.7.self_attn.q_proj', 'layers.16.self_attn.v_proj', 'layers.21.self_attn.o_proj', 'layers.22.self_attn.o_proj', 'layers.9.self_attn.v_proj', 'layers.13.self_attn.o_proj', 'layers.0.self_attn.k_proj', 'layers.4.mlp.down_proj', 'layers.0.self_attn.v_proj', 'layers.3.mlp.gate_proj', 'layers.13.mlp.down_proj', 'layers.0.self_attn.o_proj', 'layers.21.self_attn.q_proj', 'layers.23.self_attn.k_proj', 'layers.5.self_attn.q_proj', 'layers.11.self_attn.k_proj', 'layers.22.mlp.down_proj', 'layers.1.mlp.gate_proj', 'layers.17.self_attn.k_proj', 'layers.23.mlp.down_proj', 'layers.10.self_attn.v_proj', 'layers.2.self_attn.k_proj', 'layers.3.self_attn.o_proj', 'layers.18.mlp.gate_proj', 'layers.23.self_attn.o_proj', 'layers.10.self_attn.q_proj', 'layers.0.self_attn.q_proj', 'layers.16.self_attn.k_proj', 'layers.1.mlp.down_proj', 'layers.13.mlp.up_proj', 'layers.19.mlp.up_proj', 'layers.13.self_attn.q_proj', 'layers.8.self_attn.q_proj', 'layers.4.self_attn.k_proj', 'layers.5.self_attn.v_proj', 'layers.22.mlp.gate_proj', 'layers.12.self_attn.v_proj', 'layers.15.self_attn.v_proj', 'layers.9.self_attn.q_proj', 'layers.21.mlp.down_proj', 'layers.19.self_attn.k_proj', 'layers.10.self_attn.k_proj', 'layers.17.mlp.down_proj', 'layers.11.self_attn.q_proj', 'layers.9.mlp.gate_proj', 'layers.15.self_attn.o_proj', 'layers.13.mlp.gate_proj', 'layers.18.mlp.up_proj', 'layers.12.self_attn.q_proj', 'layers.15.mlp.down_proj', 'layers.23.self_attn.v_proj', 'layers.12.mlp.down_proj', 'layers.9.mlp.up_proj', 'layers.23.mlp.gate_proj', 'layers.8.mlp.down_proj', 'layers.16.mlp.down_proj', 'layers.18.self_attn.q_proj', 'layers.19.self_attn.o_proj', 'layers.23.self_attn.q_proj', 'layers.15.self_attn.q_proj', 'layers.5.self_attn.o_proj', 'layers.3.mlp.up_proj', 'layers.8.self_attn.o_proj', 'layers.3.self_attn.v_proj', 'layers.9.self_attn.o_proj', 'layers.5.self_attn.k_proj', 'layers.8.self_attn.k_proj', 'layers.6.self_attn.o_proj', 'layers.6.mlp.gate_proj', 'layers.11.mlp.up_proj', 'layers.16.self_attn.o_proj', 'layers.15.mlp.up_proj', 'layers.18.self_attn.o_proj', 'layers.20.self_attn.k_proj', 'layers.3.self_attn.q_proj', 'layers.6.mlp.down_proj', 'layers.2.mlp.gate_proj', 'layers.2.self_attn.v_proj', 'layers.7.self_attn.k_proj', 'layers.8.mlp.up_proj', 'layers.7.mlp.gate_proj', 'layers.4.mlp.gate_proj', 'layers.6.self_attn.k_proj', 'layers.20.mlp.up_proj', 'layers.1.self_attn.v_proj', 'layers.11.mlp.down_proj', 'layers.18.self_attn.v_proj', 'layers.17.mlp.up_proj', 'layers.17.self_attn.o_proj', 'layers.1.mlp.up_proj', 'layers.22.self_attn.q_proj', 'layers.5.mlp.up_proj', 'layers.2.mlp.up_proj', 'layers.22.self_attn.k_proj', 'layers.2.self_attn.o_proj', 'layers.3.mlp.down_proj', 'layers.20.self_attn.o_proj', 'layers.12.self_attn.k_proj', 'layers.19.self_attn.q_proj', 'layers.20.self_attn.q_proj', 'layers.6.self_attn.q_proj', 'layers.15.mlp.gate_proj', 'layers.7.self_attn.o_proj', 'layers.13.self_attn.k_proj', 'layers.11.self_attn.o_proj', 'layers.1.self_attn.k_proj', 'layers.6.mlp.up_proj', 'layers.16.mlp.up_proj', 'layers.9.self_attn.k_proj', 'layers.21.self_attn.v_proj', 'layers.13.self_attn.v_proj', 'layers.5.mlp.down_proj', 'layers.20.self_attn.v_proj', 'layers.0.mlp.up_proj', 'layers.14.self_attn.q_proj', 'layers.1.self_attn.o_proj', 'layers.10.mlp.up_proj', 'layers.8.self_attn.v_proj', 'layers.9.mlp.down_proj', 'layers.18.self_attn.k_proj', 'layers.2.self_attn.q_proj', 'layers.0.mlp.gate_proj', 'layers.4.self_attn.o_proj', 'layers.7.mlp.down_proj', 'layers.14.mlp.up_proj', 'layers.22.self_attn.v_proj', 'layers.4.self_attn.q_proj', 'layers.20.mlp.gate_proj', 'layers.11.mlp.gate_proj', 'layers.21.mlp.up_proj', 'layers.7.mlp.up_proj', 'layers.14.self_attn.v_proj', 'layers.16.mlp.gate_proj', 'layers.14.mlp.gate_proj', 'layers.11.self_attn.v_proj', 'layers.1.self_attn.q_proj', 'layers.10.mlp.down_proj', 'layers.4.mlp.up_proj', 'layers.14.mlp.down_proj', 'layers.19.mlp.down_proj', 'layers.22.mlp.up_proj', 'layers.19.self_attn.v_proj', 'layers.21.self_attn.k_proj', 'layers.23.mlp.up_proj', 'layers.12.mlp.up_proj', 'layers.15.self_attn.k_proj', 'layers.3.self_attn.k_proj', 'layers.12.self_attn.o_proj', 'layers.2.mlp.down_proj', 'layers.10.mlp.gate_proj', 'layers.5.mlp.gate_proj', 'layers.17.self_attn.v_proj', 'layers.6.self_attn.v_proj'} not found in the base model. Please check the target modules and try again.
2024-05-02:20:54:27,641 INFO     [__main__.py:251] Verbosity set to INFO
2024-05-02:20:54:31,894 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-05-02:20:54:31,895 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-05-02:20:54:31,895 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-2-zephyr-1_6b', 'trust_remote_code': True, 'load_in_8bit': False, 'device': 'cuda:2', 'dtype': 'float16', 'peft_dir': '/home/nlyaly/projects/lm-evaluation-harness/cache/stablelm-2-zephyr-1_6b/opt_search_q1_wikitext2_loftq_init_R8_Ldugqkvo/23'}
2024-05-02:20:54:31,971 INFO     [huggingface.py:168] Using device 'cuda:2'
/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:122: UserWarning: 

================================================================================
WARNING: Manual override via BNB_CUDA_VERSION env variable detected!
BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64
Loading: libbitsandbytes_cuda124.so
================================================================================


  warn(
2024-05-02:20:54:34,951 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-05-02:20:54:34,951 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-05-02:20:54:34,951 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-05-02:20:54:34,951 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-05-02:20:54:34,951 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-05-02:20:54:34,951 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-05-02:20:54:37,808 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-05-02:20:54:37,861 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 830.39it/s]
2024-05-02:20:54:37,938 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  2%|▏         | 1/62 [00:00<00:13,  4.37it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5267 > 2048). Running this sequence through the model will result in indexing errors
  3%|▎         | 2/62 [00:00<00:32,  1.86it/s]  5%|▍         | 3/62 [00:01<00:24,  2.39it/s]  6%|▋         | 4/62 [00:02<00:40,  1.43it/s]  8%|▊         | 5/62 [00:02<00:30,  1.89it/s] 10%|▉         | 6/62 [00:02<00:24,  2.27it/s] 11%|█▏        | 7/62 [00:04<00:36,  1.49it/s] 13%|█▎        | 8/62 [00:04<00:29,  1.86it/s] 15%|█▍        | 9/62 [00:05<00:44,  1.18it/s] 16%|█▌        | 10/62 [00:05<00:33,  1.56it/s] 18%|█▊        | 11/62 [00:06<00:27,  1.85it/s] 19%|█▉        | 12/62 [00:07<00:36,  1.38it/s] 21%|██        | 13/62 [00:07<00:26,  1.85it/s] 23%|██▎       | 14/62 [00:07<00:21,  2.22it/s] 26%|██▌       | 16/62 [00:08<00:13,  3.30it/s] 27%|██▋       | 17/62 [00:08<00:18,  2.39it/s] 29%|██▉       | 18/62 [00:08<00:14,  2.94it/s] 31%|███       | 19/62 [00:09<00:19,  2.20it/s] 32%|███▏      | 20/62 [00:10<00:22,  1.85it/s] 34%|███▍      | 21/62 [00:11<00:24,  1.66it/s] 35%|███▌      | 22/62 [00:11<00:18,  2.11it/s] 37%|███▋      | 23/62 [00:12<00:21,  1.80it/s] 39%|███▊      | 24/62 [00:13<00:32,  1.19it/s] 40%|████      | 25/62 [00:13<00:24,  1.49it/s] 42%|████▏     | 26/62 [00:14<00:25,  1.43it/s] 44%|████▎     | 27/62 [00:16<00:33,  1.06it/s] 45%|████▌     | 28/62 [00:16<00:23,  1.42it/s] 48%|████▊     | 30/62 [00:16<00:13,  2.46it/s] 50%|█████     | 31/62 [00:17<00:15,  2.03it/s] 52%|█████▏    | 32/62 [00:17<00:12,  2.42it/s] 53%|█████▎    | 33/62 [00:18<00:14,  1.97it/s] 55%|█████▍    | 34/62 [00:18<00:11,  2.39it/s] 56%|█████▋    | 35/62 [00:19<00:16,  1.59it/s] 58%|█████▊    | 36/62 [00:20<00:20,  1.28it/s] 60%|█████▉    | 37/62 [00:20<00:14,  1.67it/s] 61%|██████▏   | 38/62 [00:22<00:21,  1.14it/s] 63%|██████▎   | 39/62 [00:23<00:19,  1.18it/s] 65%|██████▍   | 40/62 [00:24<00:20,  1.07it/s] 66%|██████▌   | 41/62 [00:25<00:23,  1.11s/it] 68%|██████▊   | 42/62 [00:26<00:17,  1.16it/s] 69%|██████▉   | 43/62 [00:26<00:12,  1.56it/s] 71%|███████   | 44/62 [00:26<00:12,  1.48it/s] 73%|███████▎  | 45/62 [00:27<00:09,  1.82it/s] 74%|███████▍  | 46/62 [00:27<00:06,  2.36it/s] 76%|███████▌  | 47/62 [00:27<00:05,  2.92it/s] 77%|███████▋  | 48/62 [00:28<00:06,  2.13it/s] 79%|███████▉  | 49/62 [00:28<00:04,  2.65it/s] 81%|████████  | 50/62 [00:28<00:03,  3.19it/s] 82%|████████▏ | 51/62 [00:28<00:03,  3.23it/s] 84%|████████▍ | 52/62 [00:29<00:03,  3.24it/s] 85%|████████▌ | 53/62 [00:29<00:02,  3.96it/s] 87%|████████▋ | 54/62 [00:29<00:01,  4.69it/s] 89%|████████▊ | 55/62 [00:29<00:01,  5.48it/s] 90%|█████████ | 56/62 [00:29<00:00,  6.23it/s] 92%|█████████▏| 57/62 [00:30<00:01,  2.92it/s] 95%|█████████▌| 59/62 [00:30<00:00,  3.60it/s] 97%|█████████▋| 60/62 [00:31<00:00,  2.51it/s] 98%|█████████▊| 61/62 [00:32<00:00,  2.03it/s]100%|██████████| 62/62 [00:32<00:00,  2.18it/s]100%|██████████| 62/62 [00:32<00:00,  1.89it/s]
hf (pretrained=stabilityai/stablelm-2-zephyr-1_6b,trust_remote_code=True,load_in_8bit=False,device=cuda:2,dtype=float16,peft_dir=/home/nlyaly/projects/lm-evaluation-harness/cache/stablelm-2-zephyr-1_6b/opt_search_q1_wikitext2_loftq_init_R8_Ldugqkvo/23), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
| Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
|--------|------:|------|-----:|---------------|------:|---|------|
|wikitext|      2|none  |     0|word_perplexity|18.3058|±  |N/A   |
|        |       |none  |     0|byte_perplexity| 1.7223|±  |N/A   |
|        |       |none  |     0|bits_per_byte  | 0.7843|±  |N/A   |

2024-05-02:20:56:58,100 INFO     [__main__.py:251] Verbosity set to INFO
2024-05-02:20:57:02,280 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-05-02:20:57:02,281 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-05-02:20:57:02,281 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-2-zephyr-1_6b', 'trust_remote_code': True, 'load_in_8bit': False, 'device': 'cuda:2', 'dtype': 'float16'}
2024-05-02:20:57:02,382 INFO     [huggingface.py:168] Using device 'cuda:2'
Traceback (most recent call last):
  File "/home/nlyaly/env/lm-eval-main/bin/lm_eval", line 33, in <module>
    sys.exit(load_entry_point('lm-eval', 'console_scripts', 'lm_eval')())
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/__main__.py", line 341, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/evaluator.py", line 180, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 207, in __init__
    self._create_model(
  File "/home/nlyaly/projects/lm-eval-2/lm_eval/models/huggingface.py", line 547, in _create_model
    peft_dir = model_kwargs["peft_dir"]
KeyError: 'peft_dir'
2024-05-02:20:58:16,722 INFO     [__main__.py:251] Verbosity set to INFO
2024-05-02:20:58:21,117 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-05-02:20:58:21,118 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-05-02:20:58:21,118 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-2-zephyr-1_6b', 'trust_remote_code': True, 'load_in_8bit': False, 'device': 'cuda:2', 'dtype': 'float16'}
2024-05-02:20:58:21,206 INFO     [huggingface.py:168] Using device 'cuda:2'
2024-05-02:20:58:23,142 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-05-02:20:58:23,142 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-05-02:20:58:23,142 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-05-02:20:58:23,142 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-05-02:20:58:23,142 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-05-02:20:58:23,142 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-05-02:20:58:25,997 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-05-02:20:58:26,041 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 852.76it/s]
2024-05-02:20:58:26,116 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  2%|▏         | 1/62 [00:00<00:16,  3.70it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5267 > 2048). Running this sequence through the model will result in indexing errors
  3%|▎         | 2/62 [00:00<00:25,  2.38it/s]  5%|▍         | 3/62 [00:00<00:18,  3.20it/s]  6%|▋         | 4/62 [00:01<00:29,  1.99it/s]  8%|▊         | 5/62 [00:01<00:21,  2.66it/s] 10%|▉         | 6/62 [00:02<00:17,  3.21it/s] 11%|█▏        | 7/62 [00:02<00:25,  2.13it/s] 13%|█▎        | 8/62 [00:03<00:20,  2.65it/s] 15%|█▍        | 9/62 [00:04<00:31,  1.69it/s] 16%|█▌        | 10/62 [00:04<00:23,  2.23it/s] 18%|█▊        | 11/62 [00:04<00:19,  2.66it/s] 19%|█▉        | 12/62 [00:05<00:25,  1.99it/s] 23%|██▎       | 14/62 [00:05<00:15,  3.06it/s] 26%|██▌       | 16/62 [00:05<00:10,  4.32it/s] 27%|██▋       | 17/62 [00:06<00:13,  3.35it/s] 31%|███       | 19/62 [00:06<00:12,  3.33it/s] 32%|███▏      | 20/62 [00:07<00:14,  2.86it/s] 34%|███▍      | 21/62 [00:07<00:16,  2.55it/s] 35%|███▌      | 22/62 [00:08<00:12,  3.13it/s] 37%|███▋      | 23/62 [00:08<00:14,  2.67it/s] 39%|███▊      | 24/62 [00:09<00:21,  1.77it/s] 40%|████      | 25/62 [00:09<00:16,  2.19it/s] 42%|████▏     | 26/62 [00:10<00:17,  2.09it/s] 44%|████▎     | 27/62 [00:11<00:22,  1.54it/s] 47%|████▋     | 29/62 [00:11<00:12,  2.64it/s] 50%|█████     | 31/62 [00:12<00:10,  2.96it/s] 52%|█████▏    | 32/62 [00:12<00:08,  3.44it/s] 53%|█████▎    | 33/62 [00:12<00:10,  2.87it/s] 55%|█████▍    | 34/62 [00:12<00:08,  3.44it/s] 56%|█████▋    | 35/62 [00:13<00:11,  2.34it/s] 58%|█████▊    | 36/62 [00:14<00:13,  1.89it/s] 60%|█████▉    | 37/62 [00:14<00:10,  2.43it/s] 61%|██████▏   | 38/62 [00:15<00:14,  1.66it/s] 63%|██████▎   | 39/62 [00:16<00:13,  1.71it/s] 65%|██████▍   | 40/62 [00:16<00:14,  1.54it/s] 66%|██████▌   | 41/62 [00:18<00:16,  1.29it/s] 68%|██████▊   | 42/62 [00:18<00:12,  1.67it/s] 71%|███████   | 44/62 [00:18<00:08,  2.15it/s] 73%|███████▎  | 45/62 [00:18<00:06,  2.54it/s] 76%|███████▌  | 47/62 [00:19<00:04,  3.69it/s] 77%|███████▋  | 48/62 [00:19<00:04,  3.02it/s] 79%|███████▉  | 49/62 [00:19<00:03,  3.61it/s] 81%|████████  | 50/62 [00:19<00:02,  4.27it/s] 82%|████████▏ | 51/62 [00:20<00:02,  4.37it/s] 84%|████████▍ | 52/62 [00:20<00:02,  4.47it/s] 87%|████████▋ | 54/62 [00:20<00:01,  6.28it/s] 90%|█████████ | 56/62 [00:20<00:00,  7.98it/s] 92%|█████████▏| 57/62 [00:21<00:01,  4.64it/s] 95%|█████████▌| 59/62 [00:21<00:00,  5.38it/s] 97%|█████████▋| 60/62 [00:22<00:00,  3.83it/s] 98%|█████████▊| 61/62 [00:22<00:00,  3.07it/s]100%|██████████| 62/62 [00:22<00:00,  3.25it/s]100%|██████████| 62/62 [00:22<00:00,  2.72it/s]
hf (pretrained=stabilityai/stablelm-2-zephyr-1_6b,trust_remote_code=True,load_in_8bit=False,device=cuda:2,dtype=float16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
| Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
|--------|------:|------|-----:|---------------|------:|---|------|
|wikitext|      2|none  |     0|word_perplexity|16.8258|±  |N/A   |
|        |       |none  |     0|byte_perplexity| 1.6954|±  |N/A   |
|        |       |none  |     0|bits_per_byte  | 0.7616|±  |N/A   |

2024-05-02:20:59:40,070 INFO     [__main__.py:251] Verbosity set to INFO
2024-05-02:20:59:44,252 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-05-02:20:59:44,253 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-05-02:20:59:44,253 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'stabilityai/stablelm-2-zephyr-1_6b', 'trust_remote_code': True, 'load_in_8bit': False, 'device': 'cuda:2', 'dtype': 'float16'}
2024-05-02:20:59:44,338 INFO     [huggingface.py:168] Using device 'cuda:2'
/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:122: UserWarning: 

================================================================================
WARNING: Manual override via BNB_CUDA_VERSION env variable detected!
BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64
Loading: libbitsandbytes_cuda124.so
================================================================================


  warn(
2024-05-02:20:59:46,747 WARNING  [big_modeling.py:450] You shouldn't move a model that is dispatched using accelerate hooks.
2024-05-02:20:59:47,045 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-05-02:20:59:47,045 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-05-02:20:59:47,045 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-05-02:20:59:47,045 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-05-02:20:59:47,045 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-05-02:20:59:47,045 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-05-02:20:59:49,921 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-05-02:20:59:49,966 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 857.56it/s]
2024-05-02:20:59:50,040 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]  2%|▏         | 1/62 [00:00<00:11,  5.40it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5267 > 2048). Running this sequence through the model will result in indexing errors
  3%|▎         | 2/62 [00:00<00:23,  2.53it/s]  5%|▍         | 3/62 [00:00<00:17,  3.29it/s]  6%|▋         | 4/62 [00:01<00:29,  1.97it/s]  8%|▊         | 5/62 [00:01<00:21,  2.61it/s] 10%|▉         | 6/62 [00:02<00:17,  3.13it/s] 11%|█▏        | 7/62 [00:02<00:26,  2.06it/s] 13%|█▎        | 8/62 [00:03<00:21,  2.56it/s] 15%|█▍        | 9/62 [00:04<00:32,  1.63it/s] 16%|█▌        | 10/62 [00:04<00:24,  2.15it/s] 18%|█▊        | 11/62 [00:04<00:19,  2.56it/s] 19%|█▉        | 12/62 [00:05<00:26,  1.91it/s] 23%|██▎       | 14/62 [00:05<00:16,  2.93it/s] 26%|██▌       | 16/62 [00:05<00:11,  4.11it/s] 27%|██▋       | 17/62 [00:06<00:14,  3.20it/s] 31%|███       | 19/62 [00:07<00:13,  3.19it/s] 32%|███▏      | 20/62 [00:07<00:15,  2.74it/s] 34%|███▍      | 21/62 [00:08<00:16,  2.45it/s] 35%|███▌      | 22/62 [00:08<00:13,  2.99it/s] 37%|███▋      | 23/62 [00:08<00:15,  2.56it/s] 39%|███▊      | 24/62 [00:09<00:22,  1.70it/s] 40%|████      | 25/62 [00:10<00:17,  2.10it/s] 42%|████▏     | 26/62 [00:10<00:17,  2.00it/s] 44%|████▎     | 27/62 [00:11<00:23,  1.48it/s] 47%|████▋     | 29/62 [00:11<00:13,  2.50it/s] 50%|█████     | 31/62 [00:12<00:11,  2.80it/s] 52%|█████▏    | 32/62 [00:12<00:09,  3.24it/s] 53%|█████▎    | 33/62 [00:13<00:10,  2.72it/s] 55%|█████▍    | 34/62 [00:13<00:08,  3.25it/s] 56%|█████▋    | 35/62 [00:14<00:12,  2.24it/s] 58%|█████▊    | 36/62 [00:14<00:14,  1.81it/s] 60%|█████▉    | 37/62 [00:15<00:10,  2.32it/s] 61%|██████▏   | 38/62 [00:16<00:15,  1.59it/s] 63%|██████▎   | 39/62 [00:16<00:13,  1.65it/s] 65%|██████▍   | 40/62 [00:17<00:14,  1.49it/s] 66%|██████▌   | 41/62 [00:18<00:16,  1.25it/s] 68%|██████▊   | 42/62 [00:18<00:12,  1.60it/s] 71%|███████   | 44/62 [00:19<00:08,  2.07it/s] 73%|███████▎  | 45/62 [00:19<00:06,  2.44it/s] 76%|███████▌  | 47/62 [00:19<00:04,  3.52it/s] 77%|███████▋  | 48/62 [00:20<00:04,  2.90it/s] 79%|███████▉  | 49/62 [00:20<00:03,  3.46it/s] 81%|████████  | 50/62 [00:20<00:02,  4.08it/s] 82%|████████▏ | 51/62 [00:20<00:02,  4.20it/s] 84%|████████▍ | 52/62 [00:21<00:02,  4.29it/s] 87%|████████▋ | 54/62 [00:21<00:01,  5.97it/s] 90%|█████████ | 56/62 [00:21<00:00,  7.52it/s] 92%|█████████▏| 57/62 [00:22<00:01,  4.45it/s] 95%|█████████▌| 59/62 [00:22<00:00,  5.15it/s] 97%|█████████▋| 60/62 [00:22<00:00,  3.70it/s] 98%|█████████▊| 61/62 [00:23<00:00,  2.98it/s]100%|██████████| 62/62 [00:23<00:00,  3.15it/s]100%|██████████| 62/62 [00:23<00:00,  2.62it/s]
hf (pretrained=stabilityai/stablelm-2-zephyr-1_6b,trust_remote_code=True,load_in_8bit=False,device=cuda:2,dtype=float16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
| Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
|--------|------:|------|-----:|---------------|------:|---|------|
|wikitext|      2|none  |     0|word_perplexity|21.7729|±  |N/A   |
|        |       |none  |     0|byte_perplexity| 1.7791|±  |N/A   |
|        |       |none  |     0|bits_per_byte  | 0.8311|±  |N/A   |

2024-05-02:21:20:16,964 INFO     [__main__.py:251] Verbosity set to INFO
2024-05-02:21:20:21,079 INFO     [__main__.py:335] Selected Tasks: ['wikitext']
2024-05-02:21:20:21,080 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-05-02:21:20:21,080 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'microsoft/Phi-3-mini-4k-instruct', 'trust_remote_code': True, 'load_in_8bit': False, 'device': 'cuda:2', 'dtype': 'float16'}
2024-05-02:21:20:21,166 INFO     [huggingface.py:168] Using device 'cuda:2'
/home/nlyaly/env/lm-eval-main/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:122: UserWarning: 

================================================================================
WARNING: Manual override via BNB_CUDA_VERSION env variable detected!
BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64
Loading: libbitsandbytes_cuda124.so
================================================================================


  warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]
2024-05-02:21:20:26,926 WARNING  [big_modeling.py:450] You shouldn't move a model that is dispatched using accelerate hooks.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-05-02:21:20:27,115 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-05-02:21:20:27,115 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-05-02:21:20:27,115 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
2024-05-02:21:20:27,115 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
2024-05-02:21:20:27,115 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
2024-05-02:21:20:27,115 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
Repo card metadata block was not found. Setting CardData to empty.
2024-05-02:21:20:30,004 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.
2024-05-02:21:20:30,061 INFO     [task.py:395] Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 849.59it/s]
2024-05-02:21:20:30,136 INFO     [evaluator.py:379] Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s]2024-05-02:21:20:30,157 WARNING  [logging.py:329] You are not running the flash-attention implementation, expect numerical differences.
  2%|▏         | 1/62 [00:00<00:25,  2.39it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5944 > 4096). Running this sequence through the model will result in indexing errors
  3%|▎         | 2/62 [00:03<01:41,  1.70s/it]  5%|▍         | 3/62 [00:03<01:20,  1.36s/it]  6%|▋         | 4/62 [00:07<02:17,  2.36s/it]  8%|▊         | 5/62 [00:08<01:41,  1.78s/it] 10%|▉         | 6/62 [00:09<01:24,  1.51s/it] 11%|█▏        | 7/62 [00:14<02:29,  2.72s/it] 13%|█▎        | 8/62 [00:15<01:56,  2.16s/it] 15%|█▍        | 9/62 [00:20<02:45,  3.11s/it] 16%|█▌        | 10/62 [00:21<02:01,  2.33s/it] 18%|█▊        | 11/62 [00:22<01:41,  1.99s/it] 19%|█▉        | 12/62 [00:26<02:08,  2.58s/it] 21%|██        | 13/62 [00:27<01:32,  1.89s/it] 23%|██▎       | 14/62 [00:27<01:15,  1.58s/it] 24%|██▍       | 15/62 [00:28<00:54,  1.15s/it] 26%|██▌       | 16/62 [00:28<00:45,  1.01it/s] 27%|██▋       | 17/62 [00:32<01:24,  1.87s/it] 29%|██▉       | 18/62 [00:32<01:02,  1.41s/it] 31%|███       | 19/62 [00:35<01:16,  1.77s/it] 32%|███▏      | 20/62 [00:38<01:24,  2.02s/it] 34%|███▍      | 21/62 [00:40<01:29,  2.19s/it] 35%|███▌      | 22/62 [00:41<01:06,  1.66s/it] 37%|███▋      | 23/62 [00:43<01:15,  1.94s/it] 39%|███▊      | 24/62 [00:48<01:51,  2.93s/it] 40%|████      | 25/62 [00:49<01:26,  2.34s/it] 42%|████▏     | 26/62 [00:53<01:41,  2.82s/it] 44%|████▎     | 27/62 [00:59<02:03,  3.54s/it] 45%|████▌     | 28/62 [00:59<01:27,  2.58s/it] 48%|████▊     | 30/62 [00:59<00:45,  1.42s/it] 50%|█████     | 31/62 [01:02<00:53,  1.72s/it] 52%|█████▏    | 32/62 [01:02<00:42,  1.41s/it] 53%|█████▎    | 33/62 [01:05<00:50,  1.73s/it] 55%|█████▍    | 34/62 [01:05<00:39,  1.41s/it] 56%|█████▋    | 35/62 [01:09<00:57,  2.13s/it] 58%|█████▊    | 36/62 [01:13<01:08,  2.65s/it] 60%|█████▉    | 37/62 [01:14<00:50,  2.04s/it] 61%|██████▏   | 38/62 [01:20<01:20,  3.36s/it] 63%|██████▎   | 39/62 [01:23<01:12,  3.14s/it] 65%|██████▍   | 40/62 [01:28<01:22,  3.76s/it] 66%|██████▌   | 41/62 [01:33<01:28,  4.20s/it] 68%|██████▊   | 42/62 [01:34<01:04,  3.24s/it] 69%|██████▉   | 43/62 [01:35<00:45,  2.38s/it] 71%|███████   | 44/62 [01:37<00:44,  2.45s/it] 73%|███████▎  | 45/62 [01:38<00:33,  1.96s/it] 74%|███████▍  | 46/62 [01:39<00:23,  1.48s/it] 76%|███████▌  | 47/62 [01:39<00:17,  1.16s/it] 77%|███████▋  | 48/62 [01:43<00:27,  1.98s/it] 79%|███████▉  | 49/62 [01:43<00:19,  1.53s/it] 81%|████████  | 50/62 [01:44<00:14,  1.22s/it] 82%|████████▏ | 51/62 [01:45<00:13,  1.23s/it] 84%|████████▍ | 52/62 [01:46<00:12,  1.21s/it] 85%|████████▌ | 53/62 [01:47<00:08,  1.04it/s] 87%|████████▋ | 54/62 [01:47<00:06,  1.28it/s] 89%|████████▊ | 55/62 [01:47<00:04,  1.56it/s] 90%|█████████ | 56/62 [01:48<00:03,  1.86it/s] 92%|█████████▏| 57/62 [01:52<00:07,  1.55s/it] 94%|█████████▎| 58/62 [01:52<00:04,  1.14s/it] 95%|█████████▌| 59/62 [01:54<00:04,  1.58s/it] 97%|█████████▋| 60/62 [01:57<00:03,  1.89s/it] 98%|█████████▊| 61/62 [02:00<00:02,  2.11s/it]100%|██████████| 62/62 [02:02<00:00,  2.26s/it]100%|██████████| 62/62 [02:02<00:00,  1.98s/it]
hf (pretrained=microsoft/Phi-3-mini-4k-instruct,trust_remote_code=True,load_in_8bit=False,device=cuda:2,dtype=float16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
| Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
|--------|------:|------|-----:|---------------|------:|---|------|
|wikitext|      2|none  |     0|word_perplexity|10.9990|±  |N/A   |
|        |       |none  |     0|byte_perplexity| 1.5658|±  |N/A   |
|        |       |none  |     0|bits_per_byte  | 0.6469|±  |N/A   |

