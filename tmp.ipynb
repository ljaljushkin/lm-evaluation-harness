{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, Iterable, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "from tqdm.auto import trange\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "# from nncf import compress_weights, CompressWeightsMode, IgnoredScope\n",
    "import torch\n",
    "# import openvino as ov\n",
    "# from pathlib import Path\n",
    "# import argparse\n",
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from optimum.intel.openvino import OVModelForCausalLM\n",
    "from peft import LoftQConfig, LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# model_id = Path('/home/nlyaly/projects/lm-evaluation-harness/cache/opt-125m/lora_torch')\n",
    "# model = OVModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     export=True,\n",
    "#     trust_remote_code=True,\n",
    "#     use_cache=True,\n",
    "#     # ov_config=ov_config,\n",
    "#     stateful=False,\n",
    "#     load_in_8bit=False,\n",
    "# )\n",
    "# model.save_model()\n",
    "\n",
    "# int4_model_path = fp32_model_path.parent.parent / 'INT4'\n",
    "# int4_model_path.mkdir(exist_ok=True)\n",
    "\n",
    "# ov_model = ov.Core().read_model(fp32_model_path)\n",
    "# compressed_model = compress_weights(ov_model, mode=CompressWeightsMode.INT4_SYM, ratio=1, group_size=64, ignored_scope=IgnoredScope(patterns=[\".*lora_.*\"],))\n",
    "# ov.save_model(compressed_model, int4_model_path /'openvino_model.xml')\n",
    "\n",
    "# pretrained = 'stabilityai/stablelm-2-zephyr-1_6b'\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     pretrained,\n",
    "#     # config.base_model_name_or_path,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     # load_in_8bit=False,#load_in_8bit,\n",
    "#     # load_in_4bit=True,\n",
    "#     # low_cpu_mem_usage=low_cpu_mem_usage,\n",
    "#     # revision=revision,\n",
    "#     # torch_dtype=_get_dtype(dtype),\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map='auto'\n",
    "# )\n",
    "\n",
    "# loftq_config = LoftQConfig(\n",
    "#     loftq_bits=4,\n",
    "#     loftq_iter=1\n",
    "# )\n",
    "# lora_config = LoraConfig(\n",
    "#     init_lora_weights=\"loftq\",\n",
    "#     loftq_config=loftq_config,\n",
    "#     r=64,\n",
    "#     target_modules=[\"down_proj\"]\n",
    "# )\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# def collect_inputs():\n",
    "\n",
    "#     dataloader = get_loaders(\n",
    "#         args.dataset,\n",
    "#         nsamples=nsamples,\n",
    "#         seed=seed,\n",
    "#         model_path=model_path,\n",
    "#         seqlen=model.seqlen,\n",
    "#     )\n",
    "\n",
    "#     inps, forward_args = get_inps(model, dataloader, args)\n",
    "\n",
    "\n",
    "# def collect_outputs()\n",
    "\n",
    "# def tune():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "@contextlib.contextmanager\n",
    "def using_tf32(enabled: bool):\n",
    "    was_cudnn = torch.backends.cudnn.allow_tf32\n",
    "    was_matmul = torch.backends.cuda.matmul.allow_tf32\n",
    "    torch.backends.cudnn.allow_tf32 = enabled\n",
    "    torch.backends.cuda.matmul.allow_tf32 = enabled\n",
    "    yield\n",
    "    torch.backends.cudnn.allow_tf32 = was_cudnn\n",
    "    torch.backends.cuda.matmul.allow_tf32 = was_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_device = torch.device('cuda:1')\n",
    "def load_quantized_model():\n",
    "    base_model_dir = '/home/nlyaly/projects/lm-evaluation-harness/cache/stablelm-2-zephyr-1_6b/nf4_torch_loftq'\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_dir,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "        ),\n",
    "        device_map = nf4_device\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        base_model_dir,\n",
    "        subfolder='loftq_init',\n",
    "        is_trainable=True,\n",
    "        device=nf4_device\n",
    "    )\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fp32_lora_model():\n",
    "    base_model_dir = '/home/nlyaly/projects/lm-evaluation-harness/cache/stablelm-2-zephyr-1_6b/nf4_torch_loftq'\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_dir,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        base_model_dir,\n",
    "        subfolder='loftq_init',\n",
    "        is_trainable=True,\n",
    "    )\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from packaging import version\n",
    "from tqdm import trange\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "\n",
    "\n",
    "def set_seed(seed: Optional[int]):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "\n",
    "def get_red_pajama(nsamples, seqlen, tokenizer, eval_mode=False):\n",
    "    print(\"Loading red_pajama from togethercomputer/RedPajama-Data-1T-Sample\")\n",
    "    assert not eval_mode, \"Only train set is supported in RedPajama\"\n",
    "    traindata = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", split=\"train\")\n",
    "    tokenizer.bos_token_id = 1\n",
    "    tokenizer.eos_token_id = 2\n",
    "    trainloader = []\n",
    "    for _ in trange(nsamples, desc=\"Making red_pajama calibration set\", leave=False):\n",
    "        while True:\n",
    "            i = random.randint(0, len(traindata) - 1)\n",
    "            trainenc = tokenizer(traindata[i][\"text\"], return_tensors=\"pt\")\n",
    "            if trainenc.input_ids.shape[1] > seqlen:\n",
    "                break\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        assert inp.shape[1] == seqlen\n",
    "        trainloader.append(inp)\n",
    "    return trainloader\n",
    "\n",
    "\n",
    "def get_wikitext2(nsamples, seqlen, tokenizer, eval_mode=False):\n",
    "    if not eval_mode:\n",
    "        traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "        trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "        trainloader = []\n",
    "        for _ in range(nsamples):\n",
    "            i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "            j = i + seqlen\n",
    "            inp = trainenc.input_ids[:, i:j]\n",
    "            tar = inp.clone()\n",
    "            tar[:, :-1] = -100\n",
    "            trainloader.append((inp, tar))\n",
    "        return trainloader\n",
    "    else:\n",
    "        testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
    "        return testenc\n",
    "\n",
    "\n",
    "def get_ptb(nsamples, seqlen, tokenizer, eval_mode=False):\n",
    "    if not eval_mode:\n",
    "        traindata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"train\")\n",
    "        trainenc = tokenizer(\"\\n\\n\".join(traindata[\"sentence\"]), return_tensors=\"pt\")\n",
    "        trainloader = []\n",
    "        for _ in range(nsamples):\n",
    "            i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "            j = i + seqlen\n",
    "            inp = trainenc.input_ids[:, i:j]\n",
    "            tar = inp.clone()\n",
    "            tar[:, :-1] = -100\n",
    "            trainloader.append((inp, tar))\n",
    "        return trainloader\n",
    "    else:\n",
    "        valdata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"validation\")\n",
    "        testenc = tokenizer(\"\\n\\n\".join(valdata[\"sentence\"]), return_tensors=\"pt\")\n",
    "    return testenc\n",
    "\n",
    "\n",
    "def get_c4(nsamples, seqlen, tokenizer, eval_mode=False):\n",
    "    if not eval_mode:\n",
    "        traindata = load_dataset(\n",
    "            \"allenai/c4\",\n",
    "            \"default\",\n",
    "            data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"},\n",
    "            split=\"train\",\n",
    "            revision=\"607bd4c8450a42878aa9ddc051a65a055450ef87\",\n",
    "        )\n",
    "        trainloader = []\n",
    "        for _ in range(nsamples):\n",
    "            while True:\n",
    "                i = random.randint(0, len(traindata) - 1)\n",
    "                trainenc = tokenizer(traindata[i][\"text\"], return_tensors=\"pt\")\n",
    "                if trainenc.input_ids.shape[1] >= seqlen:\n",
    "                    break\n",
    "            i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "            j = i + seqlen\n",
    "            inp = trainenc.input_ids[:, i:j]\n",
    "            tar = inp.clone()\n",
    "            tar[:, :-1] = -100\n",
    "            trainloader.append((inp, tar))\n",
    "        return trainloader\n",
    "\n",
    "    else:\n",
    "        valdata = load_dataset(\n",
    "            \"allenai/c4\",\n",
    "            \"default\",\n",
    "            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n",
    "            split=\"validation\",\n",
    "            revision=\"607bd4c8450a42878aa9ddc051a65a055450ef87\",\n",
    "        )\n",
    "        random.seed(0)\n",
    "        valenc = []\n",
    "        for _ in range(256):\n",
    "            while True:\n",
    "                i = random.randint(0, len(valdata) - 1)\n",
    "                tmp = tokenizer(valdata[i][\"text\"], return_tensors=\"pt\")\n",
    "                if tmp.input_ids.shape[1] >= seqlen:\n",
    "                    break\n",
    "            if tmp.input_ids.shape[1] == seqlen:\n",
    "                # rare case, discovered with Yi tokenizer\n",
    "                valenc.append(tmp.input_ids)\n",
    "            else:\n",
    "                i = random.randint(0, tmp.input_ids.shape[1] - seqlen - 1)\n",
    "                j = i + seqlen\n",
    "                valenc.append(tmp.input_ids[:, i:j])\n",
    "        valenc = torch.hstack(valenc)\n",
    "        return valenc\n",
    "\n",
    "\n",
    "def get_ptb_new(nsamples, seqlen, tokenizer, eval_mode=False):\n",
    "    if not eval_mode:\n",
    "        traindata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"train\")\n",
    "        trainenc = tokenizer(\" \".join(traindata[\"sentence\"]), return_tensors=\"pt\")\n",
    "        trainloader = []\n",
    "        for _ in range(nsamples):\n",
    "            i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "            j = i + seqlen\n",
    "            inp = trainenc.input_ids[:, i:j]\n",
    "            tar = inp.clone()\n",
    "            tar[:, :-1] = -100\n",
    "            trainloader.append((inp, tar))\n",
    "        return trainloader\n",
    "    else:\n",
    "        testdata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\")\n",
    "        testenc = tokenizer(\" \".join(testdata[\"sentence\"]), return_tensors=\"pt\")\n",
    "        return testenc\n",
    "\n",
    "\n",
    "def get_c4_new(nsamples, seqlen, tokenizer, eval_mode=False):\n",
    "    if not eval_mode:\n",
    "        traindata = load_dataset(\n",
    "            \"allenai/c4\",\n",
    "            \"default\",\n",
    "            data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"},\n",
    "            split=\"train\",\n",
    "            revision=\"607bd4c8450a42878aa9ddc051a65a055450ef87\",\n",
    "        )\n",
    "        trainloader = []\n",
    "        for _ in range(nsamples):\n",
    "            while True:\n",
    "                i = random.randint(0, len(traindata) - 1)\n",
    "                trainenc = tokenizer(traindata[i][\"text\"], return_tensors=\"pt\")\n",
    "                if trainenc.input_ids.shape[1] >= seqlen:\n",
    "                    break\n",
    "            i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "            j = i + seqlen\n",
    "            inp = trainenc.input_ids[:, i:j]\n",
    "            tar = inp.clone()\n",
    "            tar[:, :-1] = -100\n",
    "            trainloader.append((inp, tar))\n",
    "        return trainloader\n",
    "    else:\n",
    "        valdata = load_dataset(\n",
    "            \"allenai/c4\",\n",
    "            \"default\",\n",
    "            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n",
    "            split=\"validation\",\n",
    "            revision=\"607bd4c8450a42878aa9ddc051a65a055450ef87\",\n",
    "        )\n",
    "        valenc = tokenizer(\" \".join(valdata[:1100][\"text\"]), return_tensors=\"pt\")\n",
    "        valenc = valenc.input_ids[:, : (256 * seqlen)]\n",
    "        return valenc\n",
    "\n",
    "\n",
    "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, eval_mode=False, model_path=None):\n",
    "    \"\"\"\n",
    "    Loads and prepares data for a Transformers model.\n",
    "    Args:\n",
    "        name (str): The name of the dataset to load.\n",
    "        This can be one of 'wikitext2', 'c4', 'ptb','pajama' for datasets loaded from Huggingface datasets,\n",
    "        or 'none' for cases where a dataset is not needed, like RTN. It can also accept data path to custom file.\n",
    "        nsamples (int, optional): The number of samples to load from the dataset. Defaults to 128.\n",
    "        seed (int, optional): The random seed value for data shuffling and splitting. Defaults to 0.\n",
    "        seqlen (int, optional): The maximum sequence length for input tokenization. Defaults to 2048.\n",
    "        model_path (str, optional): The path to the pretrained model weights or full model name.\n",
    "            used to detect llama to call proper tokenizer.\n",
    "            see https://github.com/huggingface/transformers/issues/22222#issuecomment-1488578722 for reasons.\n",
    "        eval_mode (bool, optional). defines slice selection for 'wikitext2', 'c4', 'ptb' datasets.\n",
    "        leave False for train slice.\n",
    "    Returns:\n",
    "        data (torch.utils.data.DataLoader or iterable): Data iterable for the dataset.\n",
    "    Note:\n",
    "        the popular decapoda-research Llama models have errors in tokenizer config, specifically\n",
    "        incorrect token ids for BOS, EOS. This gets corrected to ensure compatibility with transformers\n",
    "        of versions 4.29 and above.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    # for pre-tokenized datasets\n",
    "\n",
    "    if name.lower() == \"none\":\n",
    "        print(\"Not loading any dataset. (OK if you use no compression or methods like RTN.)\")\n",
    "        return None\n",
    "    elif os.path.isfile(name):\n",
    "        try:\n",
    "            data = torch.load(name)[:nsamples]\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Failed to load custom data from {name}.\",\n",
    "                \"Check data path or use one of [c4, wikitext2, ptb, pajama, none]\",\n",
    "            )\n",
    "    else:\n",
    "        # for datasets requiring tokenization\n",
    "        if \"llama\" in model_path.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "            # fix for transformer 4.28.0.dev0 compatibility\n",
    "            if tokenizer.bos_token_id != 1 or tokenizer.eos_token_id != 2:\n",
    "                try:\n",
    "                    tokenizer.bos_token_id = 1\n",
    "                    tokenizer.eos_token_id = 2\n",
    "                    print(f\"bos/eos tokens updated: {tokenizer.bos_token_id=},  {tokenizer.eos_token_id=}\")\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                    print(f\"bos/eos tokens unchanged: {tokenizer.bos_token_id=},  {tokenizer.eos_token_id=}\")\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "        if name.lower() == \"wikitext2\":\n",
    "            data = get_wikitext2(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"pajama\":\n",
    "            data = get_red_pajama(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"ptb\":\n",
    "            data = get_ptb(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"ptb_new\":\n",
    "            data = get_ptb_new(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"c4\":\n",
    "            data = get_c4(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"c4_new\":\n",
    "            data = get_c4_new(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Failed to load data from {name}.\",\n",
    "                \"Check dataset name or path or use one of [c4, wikitext2, ptb, pajama, none]\",\n",
    "            )\n",
    "\n",
    "    if hasattr(data, \"input_ids\"):\n",
    "        data = data.input_ids\n",
    "\n",
    "    print(f\"Loaded data from {name}; {len(data)=} sequences\")\n",
    "    return data\n",
    "\n",
    "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, eval_mode=False, model_path=None):\n",
    "    \"\"\"\n",
    "    Loads and prepares data for a Transformers model.\n",
    "    Args:\n",
    "        name (str): The name of the dataset to load.\n",
    "        This can be one of 'wikitext2', 'c4', 'ptb','pajama' for datasets loaded from Huggingface datasets,\n",
    "        or 'none' for cases where a dataset is not needed, like RTN. It can also accept data path to custom file.\n",
    "        nsamples (int, optional): The number of samples to load from the dataset. Defaults to 128.\n",
    "        seed (int, optional): The random seed value for data shuffling and splitting. Defaults to 0.\n",
    "        seqlen (int, optional): The maximum sequence length for input tokenization. Defaults to 2048.\n",
    "        model_path (str, optional): The path to the pretrained model weights or full model name.\n",
    "            used to detect llama to call proper tokenizer.\n",
    "            see https://github.com/huggingface/transformers/issues/22222#issuecomment-1488578722 for reasons.\n",
    "        eval_mode (bool, optional). defines slice selection for 'wikitext2', 'c4', 'ptb' datasets.\n",
    "        leave False for train slice.\n",
    "    Returns:\n",
    "        data (torch.utils.data.DataLoader or iterable): Data iterable for the dataset.\n",
    "    Note:\n",
    "        the popular decapoda-research Llama models have errors in tokenizer config, specifically\n",
    "        incorrect token ids for BOS, EOS. This gets corrected to ensure compatibility with transformers\n",
    "        of versions 4.29 and above.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    # for pre-tokenized datasets\n",
    "\n",
    "    if name.lower() == \"none\":\n",
    "        print(\"Not loading any dataset. (OK if you use no compression or methods like RTN.)\")\n",
    "        return None\n",
    "    elif os.path.isfile(name):\n",
    "        try:\n",
    "            data = torch.load(name)[:nsamples]\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Failed to load custom data from {name}.\",\n",
    "                \"Check data path or use one of [c4, wikitext2, ptb, pajama, none]\",\n",
    "            )\n",
    "    else:\n",
    "        # for datasets requiring tokenization\n",
    "        if \"llama\" in model_path.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "            # fix for transformer 4.28.0.dev0 compatibility\n",
    "            if tokenizer.bos_token_id != 1 or tokenizer.eos_token_id != 2:\n",
    "                try:\n",
    "                    tokenizer.bos_token_id = 1\n",
    "                    tokenizer.eos_token_id = 2\n",
    "                    print(f\"bos/eos tokens updated: {tokenizer.bos_token_id=},  {tokenizer.eos_token_id=}\")\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                    print(f\"bos/eos tokens unchanged: {tokenizer.bos_token_id=},  {tokenizer.eos_token_id=}\")\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "        if name.lower() == \"wikitext2\":\n",
    "            data = get_wikitext2(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"pajama\":\n",
    "            data = get_red_pajama(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"ptb\":\n",
    "            data = get_ptb(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"ptb_new\":\n",
    "            data = get_ptb_new(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"c4\":\n",
    "            data = get_c4(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        elif name.lower() == \"c4_new\":\n",
    "            data = get_c4_new(nsamples, seqlen, tokenizer, eval_mode=eval_mode)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Failed to load data from {name}.\",\n",
    "                \"Check dataset name or path or use one of [c4, wikitext2, ptb, pajama, none]\",\n",
    "            )\n",
    "\n",
    "    if hasattr(data, \"input_ids\"):\n",
    "        data = data.input_ids\n",
    "\n",
    "    print(f\"Loaded data from {name}; {len(data)=} sequences\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_inps(\n",
    "    model, data_iterable, seqlen, nsamples\n",
    ") -> Sequence[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    mocks model launch to collect inputs to the first model layer\n",
    "    :returns: a list of torch tensors with activations for each device in devices.\n",
    "    Each tensor has shape [nsample_per_device, seq_len, hid_size]\n",
    "    \"\"\"\n",
    "    devices = ['cuda:0']\n",
    "    offload_activations = False\n",
    "    print(\"catching layer inputs from data\", flush=True)\n",
    "\n",
    "    layers = model.model.layers#get_layers(model)\n",
    "\n",
    "    # nsamples = nsamples or args.nsamples or len(data_iterable)\n",
    "    device = devices[0] if not offload_activations else torch.device(\"cpu\")\n",
    "    assert nsamples is not None\n",
    "\n",
    "    if isinstance(data_iterable, torch.Tensor):\n",
    "        def batch_generator(testenc, seqlen, nsamples):\n",
    "            for i in range(nsamples):\n",
    "                batch = testenc[:, (i * seqlen) : ((i + 1) * seqlen)].to(device)\n",
    "                yield batch\n",
    "\n",
    "        data_iterable = batch_generator(data_iterable, seqlen, nsamples)\n",
    "\n",
    "    emb = model.get_input_embeddings()\n",
    "    emb_device = emb.weight.device\n",
    "    if emb_device.type != \"cuda\":\n",
    "        emb = emb.to(device)\n",
    "        # opt has other embeddings\n",
    "        # if model.config.model_type == \"opt\":\n",
    "        #     model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(device)\n",
    "        #     if hasattr(model.model.decoder, \"project_in\") and model.model.decoder.project_in:\n",
    "        #         model.model.decoder.project_in = model.model.decoder.project_in.to(device)\n",
    "    device = emb.weight.device  # now default device is the one where the embeddings are.\n",
    "    layer_device = next(layers[0].parameters()).device\n",
    "    layers[0] = layers[0].to(device)\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    nsamples_per_device = (nsamples - 1) // len(devices) + 1\n",
    "    inps = [\n",
    "        torch.zeros(\n",
    "            (min(nsamples_per_device, nsamples - i * nsamples_per_device), seqlen, model.config.hidden_size),\n",
    "            dtype=dtype,\n",
    "            device=devices[i] if not offload_activations else \"cpu\",\n",
    "            pin_memory=offload_activations,\n",
    "        )\n",
    "        for i in range(len(devices))\n",
    "    ]\n",
    "    forward_arg_names = [\"attention_mask\", \"position_ids\", \"use_cache\"]\n",
    "    # if model.config.model_type.lower() in FALCON_TYPES:\n",
    "    #     forward_arg_names.append(\"alibi\")\n",
    "\n",
    "    cache = {\"i\": 0, \"alibi\": None}\n",
    "\n",
    "    class CatcherExit(Exception):\n",
    "        pass\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps[cache[\"i\"] // nsamples_per_device][cache[\"i\"] % nsamples_per_device] = inp\n",
    "            cache[\"i\"] += 1\n",
    "            for forward_arg_name in forward_arg_names:\n",
    "                cache[forward_arg_name] = kwargs.get(forward_arg_name)\n",
    "            raise CatcherExit()\n",
    "\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    saved_num_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(min(16, saved_num_threads))\n",
    "    for batch_inps in data_iterable:\n",
    "        try:\n",
    "            if isinstance(batch_inps, (list, tuple)):\n",
    "                batch_inps, *_ = batch_inps\n",
    "            batch_inps = batch_inps.to(device)\n",
    "            # call model.forward to trigger the Catcher\n",
    "            model(batch_inps, attention_mask=torch.ones_like(batch_inps))\n",
    "        except CatcherExit:\n",
    "            pass  # exit after catcher finished without running the rest of the model layers\n",
    "    torch.set_num_threads(saved_num_threads)\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    layers[0] = layers[0].to(layer_device)\n",
    "    model.get_input_embeddings().to(emb_device)\n",
    "    # if model.config.model_type == \"opt\":\n",
    "    #     model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(emb_device)\n",
    "    #     if hasattr(model.model.decoder, \"project_in\") and model.model.decoder.project_in:\n",
    "    #         model.model.decoder.project_in = model.model.decoder.project_in.to(emb_device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    forward_args = {k: cache[k] for k in forward_arg_names}\n",
    "    assert cache[\"i\"] == nsamples\n",
    "    return inps, forward_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'c4'\n",
    "nsamples = 100 # 20 # TODO: 1024\n",
    "seed = 0\n",
    "model_path = 'stabilityai/stablelm-2-zephyr-1_6b'\n",
    "seqlen = 256\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3338 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from c4; len(data)=100 sequences\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_loaders(\n",
    "    dataset,\n",
    "    nsamples=nsamples,\n",
    "    seed=seed,\n",
    "    model_path=model_path,\n",
    "    seqlen=seqlen,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catching layer inputs from data\n"
     ]
    }
   ],
   "source": [
    "inps_tensor, forward_args = get_inps(model, dataloader, seqlen, nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_layer = model.model.layers[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attention mask should be of size (100, 1, 256, 256), but is torch.Size([1, 1, 256, 256])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfp32_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_args\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/transformers/models/stablelm/modeling_stablelm.py:625\u001b[0m, in \u001b[0;36mStableLmDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/transformers/models/stablelm/modeling_stablelm.py:349\u001b[0m, in \u001b[0;36mStableLmAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;241m1\u001b[39m, q_len, kv_seq_len):\n\u001b[0;32m--> 349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention mask should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39mq_len,\u001b[38;5;250m \u001b[39mkv_seq_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m         )\n\u001b[1;32m    352\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Attention mask should be of size (100, 1, 256, 256), but is torch.Size([1, 1, 256, 256])"
     ]
    }
   ],
   "source": [
    "fp32_layer(inps_tensor[0].to(device), **forward_args)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inps_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inps \u001b[38;5;241m=\u001b[39m \u001b[43minps_tensor\u001b[49m\n\u001b[1;32m      2\u001b[0m outs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mzeros_like(inp_tensor, pin_memory\u001b[38;5;241m=\u001b[39minp_tensor\u001b[38;5;241m.\u001b[39mis_pinned()) \u001b[38;5;28;01mfor\u001b[39;00m inp_tensor \u001b[38;5;129;01min\u001b[39;00m inps]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(inps_tensor[\u001b[38;5;241m0\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalc outs after quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inps_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "inps = inps_tensor\n",
    "outs = [torch.zeros_like(inp_tensor, pin_memory=inp_tensor.is_pinned()) for inp_tensor in inps]\n",
    "for j in trange(len(inps_tensor[0]), desc=\"calc outs after quantization\", leave=False):\n",
    "    outs_batch = fp32_layer(inps[0][j].to(device).unsqueeze(0), **forward_args)[0]\n",
    "    outs[0][j].copy_(outs_batch.reshape_as(outs[0][j]), non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 256, 2048])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(outs) == len(inps)\n",
    "inps[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,796,480 || all params: 1,656,311,808 || trainable%: 0.7122137234681841\n"
     ]
    }
   ],
   "source": [
    "nf4_model = load_quantized_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_model = nf4_model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_layer = nf4_model.model.model.layers[0]\n",
    "nf4_layer = nf4_layer.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterator, Optional, Sequence\n",
    "def iterate_minibatches(\n",
    "    *tensors: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    allow_incomplete: bool = True,\n",
    "    device: Optional[torch.device] = None,\n",
    "    callback: Callable[[Sequence[torch.Tensor]], Sequence[torch.Tensor]] = lambda x: x,\n",
    ") -> Iterator[Sequence[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Samples data points *forever*, in random order, with less overhead than DataLoader;\n",
    "    Adapted from https://github.com/stanis-morozov/unq/blob/master/lib/utils.py\n",
    "    probably implemented over9000 times in transformers, torch, etc\n",
    "    :param tensors: one or more tensors with the same 0-th dimension\n",
    "    :param batch_size: sample this many points with each yield\n",
    "    :param allow_incomplete: if True and if dataset size is not divisible by batch size, the last batch\n",
    "        may have less than :batch_size: samples to cover the entire dataset. If False, the last batch is dropped\n",
    "    :param callback: optional function to be called on each batch of tensors before it is yielded to the user\n",
    "    :returns: generates a tuple of minibatches from each tensor, same length as input *tensors\n",
    "        If a batch contains only one tensor, this function will yield a tensor (and not a tuple/list with one tensor)\n",
    "    \"\"\"\n",
    "    num_samples = len(tensors[0])\n",
    "    assert all(len(x) == num_samples for x in tensors)\n",
    "    indices = torch.randperm(num_samples, device=tensors[0].device)\n",
    "    while True:\n",
    "        prev_batch = None\n",
    "        for batch_start in range(0, len(indices), batch_size):\n",
    "            if not allow_incomplete and batch_start + batch_size > len(indices):\n",
    "                break\n",
    "            batch_ix = indices[batch_start : batch_start + batch_size]\n",
    "            batch = callback(tuple(tensor[batch_ix].to(device, non_blocking=True) for tensor in tensors))\n",
    "            if prev_batch is not None:\n",
    "                yield prev_batch\n",
    "            prev_batch = batch if isinstance(batch, (list, tuple)) and len(tensors) > 1 else batch[0]\n",
    "            del batch\n",
    "        yield prev_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.enable_grad()\n",
    "def finetune_groupwise(\n",
    "    *,\n",
    "    layer: nn.Module,\n",
    "    inps: Sequence[torch.Tensor],\n",
    "    outs: Sequence[torch.Tensor],\n",
    "    devices = [torch.device(\"cuda:1\")],\n",
    "    offload_activations = False,\n",
    "    verbose: bool = True,\n",
    "    **kwargs,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Fine-tune a module with pre-quantized linear layers so as to minimize MSE between layer-wise inps/outs\n",
    "\n",
    "    :param layer: a trainable module where linear layers are replaced by QuantizedLinear instances\n",
    "    :param inps: a list of tensors of input activations, [nsamples_per_device, seq_len, hidden_size]\n",
    "    :param outs: a list of tensors of previous output activations, [nsamples_per_device, seq_len, hidden_size]\n",
    "    :param args: quantization hyperparameters from main.py\n",
    "    :param kwargs: additional keyword arguments to be passed into layer on each forward\n",
    "    \"\"\"\n",
    "    # lr =1e-4\n",
    "    finetune_lr = 1e-5\n",
    "    finetune_adam_beta1 = 0.9\n",
    "    finetune_adam_beta2 = 0.95\n",
    "    finetune_batch_size = 32\n",
    "    relative_mse_tolerance = 0.01\n",
    "    finetune_relative_mse_tolerance = 0.001\n",
    "    local_batch_size = 1 # TODO: ???\n",
    "    finetune_max_epochs = 1000\n",
    "    print_frequency = 1\n",
    "\n",
    "    assert isinstance(devices, (list, tuple)) and len(devices) >= 1, f\"Found devices = {devices}\"\n",
    "    assert isinstance(inps, (list, tuple)) and isinstance(inps, (list, tuple))\n",
    "    assert len(inps) == len(outs) == len(devices)\n",
    "    for i in range(len(devices)):\n",
    "        assert isinstance(inps[i], torch.Tensor) and isinstance(outs[i], torch.Tensor)\n",
    "        if not offload_activations:\n",
    "            assert inps[i].device == outs[i].device == devices[i], (inps[i].device, outs[i].device, devices)\n",
    "        else:\n",
    "            assert inps[i].device == outs[i].device == torch.device(\"cpu\")\n",
    "            assert inps[i].is_pinned() and outs[i].is_pinned()\n",
    "\n",
    "    # replicate non-trainable parameters to each GPU\n",
    "    replicas = kwargs_by_device = None\n",
    "    # if len(devices) > 1:\n",
    "    #     replicas = torch.nn.parallel.replicate(layer, devices)\n",
    "    #     replicas[0] = layer\n",
    "    #     kwargs_by_device = []\n",
    "    #     for device in devices:\n",
    "    #         kwargs_by_device.append(\n",
    "    #             {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v) for k, v in kwargs.items()}\n",
    "    #         )\n",
    "\n",
    "    # initialize trainable parameters on main device; prepare to send them to replicas\n",
    "    differentiable_parameters_by_name = {name: param for name, param in layer.named_parameters() if param.requires_grad}\n",
    "    param_names, differentiable_parameters = zip(*differentiable_parameters_by_name.items())\n",
    "    differentiable_parameters = nn.ParameterList(differentiable_parameters)\n",
    "    for param in differentiable_parameters:\n",
    "        param.grad = torch.zeros_like(param)\n",
    "    if replicas:\n",
    "        replacement_tables = _make_parameter_replacement_tables(layer, replicas, param_names, differentiable_parameters)\n",
    "\n",
    "    print(f\"Fine-tuning {sum(param.numel() for param in differentiable_parameters)} parameters\")\n",
    "    opt = torch.optim.Adam(\n",
    "        differentiable_parameters, lr=finetune_lr, betas=(finetune_adam_beta1, finetune_adam_beta2)\n",
    "    )\n",
    "\n",
    "    # backup best parameters\n",
    "    # if args.finetune_keep_best:\n",
    "    #     best_parameters = deepcopy(differentiable_parameters)\n",
    "\n",
    "    assert finetune_batch_size % len(devices) == 0, \"batch_size must be divisible by the number of GPUs\"\n",
    "\n",
    "    num_samples_per_device = len(inps[0])\n",
    "    local_batch_size = local_batch_size\n",
    "    if local_batch_size is None:\n",
    "        local_batch_size = finetune_batch_size // len(devices)\n",
    "\n",
    "    assert all(len(inps_tensor) == num_samples_per_device for inps_tensor in inps)\n",
    "    assert finetune_batch_size % (local_batch_size * len(devices)) == 0, \"\"\n",
    "    num_accumulation_steps = finetune_batch_size // (local_batch_size * len(devices))\n",
    "    assert num_samples_per_device % local_batch_size * num_accumulation_steps == 0, (\n",
    "        num_samples_per_device,\n",
    "        local_batch_size,\n",
    "    )\n",
    "    steps_per_epoch = num_samples_per_device * len(devices) // finetune_batch_size\n",
    "    batch_iterators = [\n",
    "        iterate_minibatches(inps[i], outs[i], batch_size=local_batch_size, device=devices[i])\n",
    "        for i in range(len(devices))\n",
    "    ]\n",
    "\n",
    "    previous_best_loss = float(\"inf\")  # for early stopping\n",
    "    steps_accumulated = 0\n",
    "    for epoch in range(finetune_max_epochs):\n",
    "        loss_numerator = loss_denominator = 0\n",
    "        for step in range(steps_per_epoch):\n",
    "            if len(devices) == 1:\n",
    "                loss = _compute_mse_on_batch(layer, batch_iterators[0], **kwargs)\n",
    "            # else:\n",
    "            #     loss = _compute_mse_parallel(\n",
    "            #         devices,\n",
    "            #         replicas,\n",
    "            #         differentiable_parameters,\n",
    "            #         replacement_tables,\n",
    "            #         batch_iterators,\n",
    "            #         kwargs_by_device,\n",
    "            #     )\n",
    "\n",
    "            (loss / num_accumulation_steps).backward()\n",
    "            steps_accumulated += 1\n",
    "\n",
    "            if not torch.isfinite(loss).item():\n",
    "                raise ValueError(f\"Fine-tuning loss is {loss}\")\n",
    "            if steps_accumulated >= num_accumulation_steps:\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                steps_accumulated = 0\n",
    "\n",
    "            loss_numerator += loss.item()\n",
    "            loss_denominator += 1\n",
    "            if verbose and (epoch * steps_per_epoch + step) % print_frequency == 0:\n",
    "                print(f\"epoch={epoch}\\tstep={step}\\tloss={loss_numerator / loss_denominator:.10f}\\t\")\n",
    "\n",
    "        if verbose and (epoch * steps_per_epoch + step) % print_frequency != 0:\n",
    "            print(f\"epoch={epoch}\\tstep={step}\\tloss={loss_numerator / loss_denominator:.10f}\\t\")\n",
    "\n",
    "        if finetune_relative_mse_tolerance is not None:\n",
    "            epoch_loss = loss_numerator / loss_denominator\n",
    "            # if args.finetune_keep_best:\n",
    "            #     if epoch_loss / previous_best_loss < 1.0:\n",
    "            #         best_parameters = deepcopy(differentiable_parameters)\n",
    "            #     else:\n",
    "            #         differentiable_parameters = best_parameters\n",
    "            if epoch_loss / previous_best_loss > (1.0 - finetune_relative_mse_tolerance):\n",
    "                return layer  # early stopping; no updates after last epoch's beam search\n",
    "            previous_best_loss = min(epoch_loss, previous_best_loss)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[0] = outs[0].to('cuda:1')\n",
    "inps[0] = inps[0].to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _compute_mse_on_batch(\n",
    "    layer: nn.Module, batch_iter: Iterator[Tuple[torch.Tensor, torch.Tensor]], **kwargs\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the activation MSE error between transformer layers\n",
    "    :param\n",
    "    \"\"\"\n",
    "    inps_batch, outs_batch = next(batch_iter)\n",
    "    inps_batch = inps_batch.to(dtype=torch.float32)\n",
    "    outs_batch = outs_batch.to(dtype=torch.float32)\n",
    "\n",
    "    if inps_batch.shape[0] != 1:  # replicate kwargs to match the batch size\n",
    "        for name, value in list(kwargs.items()):\n",
    "            if isinstance(value, torch.Tensor) and value.shape[0] == 1:\n",
    "                if name not in (\"attention_mask\", \"position_ids\"):\n",
    "                    warnings.warn(f\"Tiling an unexpected kwarg {name} over batch size; make sure this is valid.\")\n",
    "                repeats = [len(inps_batch)] + [1 for _ in range(value.ndim - 1)]\n",
    "                kwargs[name] = value.tile(*repeats)\n",
    "\n",
    "    outs_prediction, *_unused = layer(inps_batch, **kwargs)\n",
    "    assert outs_prediction.shape == outs_batch.shape\n",
    "    return F.mse_loss(outs_prediction, outs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf4_layer.self_attn.q_proj.weight.device\n",
    "# inps[0].device\n",
    "# outs[0].device\n",
    "# forward_args = {name: value.to('cuda:1') for name, value in forward_args.items() if isinstance(value, torch.Tensor)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{device(type='cuda', index=1)}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{param.device for name, param in forward_args.items()}\n",
    "# inps[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning 491520 parameters\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nf4_layer \u001b[38;5;241m=\u001b[39m nf4_layer\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m using_tf32(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnf4_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 109\u001b[0m, in \u001b[0;36mfinetune_groupwise\u001b[0;34m(layer, inps, outs, devices, offload_activations, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     loss \u001b[38;5;241m=\u001b[39m _compute_mse_parallel(\n\u001b[1;32m    101\u001b[0m         devices,\n\u001b[1;32m    102\u001b[0m         replicas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m         kwargs_by_device,\n\u001b[1;32m    107\u001b[0m     )\n\u001b[0;32m--> 109\u001b[0m \u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_accumulation_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m steps_accumulated \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(loss)\u001b[38;5;241m.\u001b[39mitem():\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lm-eval-fresh/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "nf4_layer = nf4_layer.to(dtype=torch.float32)\n",
    "with using_tf32(enabled=True):\n",
    "    layer = finetune_groupwise(layer=nf4_layer, inps=inps, outs=outs, **forward_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableLmDecoderLayer(\n",
       "  (self_attn): StableLmAttention(\n",
       "    (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "    (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "    (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "    (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): StableLmRotaryEmbedding()\n",
       "  )\n",
       "  (mlp): StableLmMLP(\n",
       "    (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "    (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "    (down_proj): lora.Linear4bit(\n",
       "      (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Identity()\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=5632, out_features=64, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf4_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
